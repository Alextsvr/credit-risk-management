{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf410992",
   "metadata": {},
   "source": [
    "## 01. Library import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102a61b0",
   "metadata": {},
   "source": [
    "**Purpose**: IO/compute stack, deterministic seed, clean logs & version snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1751f381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 2.2.2 | numpy: 1.26.4 | pyarrow: 21.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_bool_dtype, is_integer_dtype, is_float_dtype\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Clean logs\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pyarrow\")\n",
    "\n",
    "# Pandas display (for debugging)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "# Versions snapshot\n",
    "def lib_versions() -> Dict[str, str]:\n",
    "    return {\n",
    "        \"pandas\": pd.__version__,\n",
    "        \"numpy\": np.__version__,\n",
    "        \"pyarrow\": pa.__version__,\n",
    "    }\n",
    "\n",
    "print(f\"pandas: {pd.__version__} | numpy: {np.__version__} | pyarrow: {pa.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d63bc47",
   "metadata": {},
   "source": [
    "## 02. Project paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd068e96",
   "metadata": {},
   "source": [
    "**Purpose**: Define input/output folders and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2d8ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR=D:\\final_v2\\credit-risk-management\\data\\train-data\n",
      "ARTIFACTS_DIR=D:\\final_v2\\credit-risk-management\\artifacts\n",
      "REPORTS_DIR=D:\\final_v2\\credit-risk-management\\reports\n"
     ]
    }
   ],
   "source": [
    "# Project structure\n",
    "PROJECT_ROOT   = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "PROJECT_DIR    = PROJECT_ROOT / \"notebooks\"\n",
    "\n",
    "# Data & artifacts\n",
    "DATA_DIR       = (PROJECT_ROOT / \"data\" / \"train-data\").resolve()   # all raw parquet shards live here\n",
    "ARTIFACTS_DIR  = (PROJECT_ROOT / \"artifacts\").resolve()\n",
    "REPORTS_DIR    = (PROJECT_ROOT / \"reports\").resolve()\n",
    "SRC_DIR        = (PROJECT_ROOT / \"src\").resolve()\n",
    "\n",
    "# Add src/ to sys.path for imports\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "# Ensure output dirs exist\n",
    "for p in [ARTIFACTS_DIR, REPORTS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Main artifacts\n",
    "FINAL_DS_PATH = ARTIFACTS_DIR / \"final_dataset.parquet\"   # single-file parquet\n",
    "TARGET_CSV    = DATA_DIR / \"train_target.csv\"\n",
    "\n",
    "# Sanity checks\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"DATA_DIR not found: {DATA_DIR}\")\n",
    "if not TARGET_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Target CSV not found: {TARGET_CSV}\")\n",
    "\n",
    "print(f\"DATA_DIR={DATA_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR={ARTIFACTS_DIR}\")\n",
    "print(f\"REPORTS_DIR={REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b11410",
   "metadata": {},
   "source": [
    "## 03.1 Global dtype rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd312032",
   "metadata": {},
   "source": [
    "**Purpose**:  combine explicit casts from features.py with prefix-based heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48afc83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE_RULES: dict[str, list[str]] = {\n",
    "    # --- integers (counters, durations, amounts) ---\n",
    "    \"Int32\": [\n",
    "        \"pre_since_opened\", \"pre_since_confirmed\",\n",
    "        \"pre_pterm\", \"pre_fterm\",\n",
    "        \"pre_till_pclose\", \"pre_till_fclose\",\n",
    "        \"pre_loans_credit_limit\", \"pre_loans_next_pay_summ\",\n",
    "        \"pre_loans_outstanding\", \"pre_loans_total_overdue\",\n",
    "        \"pre_loans_max_overdue_sum\", \"pre_loans_credit_cost_rate\",\n",
    "        \"pre_loans5\", \"pre_loans530\", \"pre_loans3060\",\n",
    "        \"pre_loans6090\", \"pre_loans90\",\n",
    "    ],\n",
    "\n",
    "    # --- ratios (float signals) ---\n",
    "    \"Float32\": [\n",
    "        \"pre_util\", \"pre_over2limit\", \"pre_maxover2limit\",\n",
    "        \"debt_to_limit\", \"overdue_to_limit\", \"maxoverdue_to_limit\",\n",
    "        \"loan_term_ratio\", \"since_ratio\", \"till_close_gap\",\n",
    "        \"serious_delay_ratio\", \"paym_good_count\", \"paym_bad_count\",\n",
    "        \"paym_last_status\",  # added in features.py\n",
    "    ],\n",
    "\n",
    "    # --- binary flags ---\n",
    "    \"UInt8\": [\n",
    "        \"is_zero_loans5\", \"is_zero_loans530\", \"is_zero_loans3060\",\n",
    "        \"is_zero_loans6090\", \"is_zero_loans90\",\n",
    "        \"is_zero_util\", \"is_zero_over2limit\", \"is_zero_maxover2limit\",\n",
    "        \"pclose_flag\", \"fclose_flag\",\n",
    "        # all OHE dummies will also be uint8\n",
    "    ],\n",
    "\n",
    "    # --- small encoded categoricals ---\n",
    "    \"Int16\": [\n",
    "        \"enc_loans_account_holder_type\",\n",
    "        \"enc_loans_credit_status\",\n",
    "        \"enc_loans_account_cur\",\n",
    "        \"enc_loans_credit_type\",\n",
    "        \"paym_last_clean_streak\",  # added in features.py\n",
    "        # enc_paym_* will be appended dynamically\n",
    "    ],\n",
    "}\n",
    "\n",
    "def extend_dtype_rules_with_enc_paym(existing_cols: list[str]) -> None:\n",
    "    \"\"\"Append all enc_paym_* columns present in data to Int16 rules.\"\"\"\n",
    "    paym_cols = sorted([c for c in existing_cols if c.startswith(\"enc_paym_\")])\n",
    "    already = set(DTYPE_RULES.get(\"Int16\", []))\n",
    "    for c in paym_cols:\n",
    "        if c not in already:\n",
    "            DTYPE_RULES[\"Int16\"].append(c)\n",
    "            already.add(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05bdce7",
   "metadata": {},
   "source": [
    "## 03.2 Helpers — ID normalizer & dtype harmonizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f77ecd",
   "metadata": {},
   "source": [
    "**Purpose**: \n",
    "\n",
    "> Enforce consistent id column (string[pyarrow]).\n",
    "\n",
    "> Prevent accidental use of 'rn' unless explicitly allowed.\n",
    "\n",
    "> Harmonize dtypes to be Arrow-friendly (avoid ArrowTypeError)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48a09b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_id_column(\n",
    "    df: pd.DataFrame,\n",
    "    allow_rn_as_id: bool = False,\n",
    "    source: str | None = None\n",
    ") -> None:\n",
    "    \"\"\"Ensure df has 'id' column as string[pyarrow].\"\"\"\n",
    "    if \"id\" in df.columns:\n",
    "        df[\"id\"] = df[\"id\"].astype(\"string\")\n",
    "        return\n",
    "\n",
    "    for c in [\"ID\", \"Id\", \"client_id\", \"customer_id\", \"loan_id\", \"account_id\", \"user_id\", \"uid\"]:\n",
    "        if c in df.columns:\n",
    "            df[\"id\"] = df[c].astype(\"string\")\n",
    "            return\n",
    "\n",
    "    if allow_rn_as_id and \"rn\" in df.columns:\n",
    "        df[\"id\"] = df[\"rn\"].astype(\"string\")\n",
    "        df[\"_id_from_rn\"] = True\n",
    "        print(f\"Using rn as id in {source or '<df>'}. This may cause collisions/leakage.\")\n",
    "        return\n",
    "\n",
    "    raise KeyError(\n",
    "        f\"Missing 'id' in {source or '<df>'}. Columns: {list(df.columns)}. \"\n",
    "        \"Make sure READ_COLUMNS includes 'id'.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def harmonize_dtypes(\n",
    "    df: pd.DataFrame,\n",
    "    dtype_rules: dict[str, list[str]] | None = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert dtypes to Arrow-friendly formats:\n",
    "    - id → string[pyarrow]\n",
    "    - bool → UInt8\n",
    "    - int → nullable Int32/Int64\n",
    "    - object → string[pyarrow]\n",
    "    \"\"\"\n",
    "    if dtype_rules is None:\n",
    "        dtype_rules = DTYPE_RULES\n",
    "\n",
    "    managed: set[str] = {\"id\"}\n",
    "    for dt, cols in dtype_rules.items():\n",
    "        managed.update(cols)\n",
    "\n",
    "    if \"id\" in df.columns:\n",
    "        try:\n",
    "            df[\"id\"] = df[\"id\"].astype(\"string[pyarrow]\")\n",
    "        except Exception:\n",
    "            df[\"id\"] = df[\"id\"].astype(\"string\")\n",
    "\n",
    "    for col in (c for c in df.columns if c not in managed):\n",
    "        s = df[col]\n",
    "\n",
    "        if is_bool_dtype(s):\n",
    "            df[col] = s.astype(\"UInt8\")\n",
    "            continue\n",
    "\n",
    "        if is_integer_dtype(s):\n",
    "            try:\n",
    "                m, M = s.min(), s.max()\n",
    "                if (\n",
    "                    pd.api.types.is_integer(m) and \n",
    "                    pd.api.types.is_integer(M) and \n",
    "                    -2**31 <= m < 2**31 and -2**31 <= M < 2**31\n",
    "                ):\n",
    "                    df[col] = pd.to_numeric(s, errors=\"coerce\").astype(\"Int32\")\n",
    "                else:\n",
    "                    df[col] = pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "            except Exception:\n",
    "                df[col] = pd.to_numeric(s, errors=\"coerce\").astype(\"Int32\")\n",
    "            continue\n",
    "\n",
    "        if df[col].dtype == \"object\":\n",
    "            try:\n",
    "                df[col] = s.astype(\"string[pyarrow]\")\n",
    "            except Exception:\n",
    "                df[col] = s.astype(\"string\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdfc252",
   "metadata": {},
   "source": [
    "## 04. Load target CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd8cc09",
   "metadata": {},
   "source": [
    "**Purpose**: Read target once, normalize id dtype, keep column name as 'target', quick sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203b4fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target shape: (3000000, 1) | positive rate: 0.0355\n"
     ]
    }
   ],
   "source": [
    "target = pd.read_csv(\n",
    "    TARGET_CSV,\n",
    "    dtype={\"id\": \"string\", \"flag\": \"int8\"}  # enforce stable dtypes on load\n",
    ")\n",
    "\n",
    "# Expect ['id','flag'] in file\n",
    "assert {\"id\", \"flag\"} <= set(target.columns), \\\n",
    "    f\"train_target.csv must contain ['id','flag'], got {list(target.columns)}\"\n",
    "\n",
    "# Normalize to 'target'\n",
    "target = target.rename(columns={\"flag\": \"target\"})\n",
    "target = target.set_index(\"id\")\n",
    "\n",
    "# Sanity checks\n",
    "pos_rate = float(target[\"target\"].mean())\n",
    "print(f\"Target shape: {target.shape} | positive rate: {pos_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ee72d",
   "metadata": {},
   "source": [
    "## 05. Import feature pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af718814",
   "metadata": {},
   "source": [
    "**Purpose**: Reuse the exact same FeatureGenerator from src/utils/features.py as in notebook 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fbef3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.features import FeatureGenerator, FeatureConfig\n",
    "\n",
    "fg = FeatureGenerator(FeatureConfig())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf091c3",
   "metadata": {},
   "source": [
    "## 05.1 FeatureGenerator contract check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f6385b",
   "metadata": {},
   "source": [
    "**Purpose**: Ensure FG is stateless and does not require .fit()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29102602",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hasattr(fg, \"transform\") and callable(fg.transform), \"FeatureGenerator must implement .transform(df)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e81a3",
   "metadata": {},
   "source": [
    "## 06. Locate raw parquet shards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede3afd",
   "metadata": {},
   "source": [
    "**Purpose**: Collect and sort all source parquet files to stream through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21307a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found shards: 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WindowsPath('D:/final_v2/credit-risk-management/data/train-data/train_data_0.pq'),\n",
       " WindowsPath('D:/final_v2/credit-risk-management/data/train-data/train_data_1.pq'),\n",
       " WindowsPath('D:/final_v2/credit-risk-management/data/train-data/train_data_10.pq'),\n",
       " WindowsPath('D:/final_v2/credit-risk-management/data/train-data/train_data_11.pq'),\n",
       " WindowsPath('D:/final_v2/credit-risk-management/data/train-data/train_data_2.pq')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_files: List[Path] = sorted(DATA_DIR.glob(\"*.pq\"))\n",
    "assert parquet_files, \"No parquet shards found in data/\"\n",
    "\n",
    "print(\"Found shards:\", len(parquet_files))\n",
    "parquet_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c357bc8",
   "metadata": {},
   "source": [
    "## 07. Choose input columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1742e8",
   "metadata": {},
   "source": [
    "**Purpose**: Auto-build a minimal column whitelist from shard schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75c88a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column whitelist built: 42 columns → D:\\final_v2\\credit-risk-management\\artifacts\\feature_whitelist.json\n"
     ]
    }
   ],
   "source": [
    "AUTO_BUILD_WHITELIST = True\n",
    "\n",
    "if AUTO_BUILD_WHITELIST:\n",
    "    # peek schema from first shard\n",
    "    schema_cols = set(pq.ParquetFile(parquet_files[0]).schema.names)\n",
    "    cfg = FeatureConfig()  # from utils.features\n",
    "\n",
    "    # explicitly required columns\n",
    "    needed_exact = {\n",
    "        \"id\", \"rn\",\n",
    "        \"pre_loans_credit_limit\", \"pre_loans_outstanding\", \"pre_loans_total_overdue\", \"pre_loans_max_overdue_sum\",\n",
    "        \"pre_pterm\", \"pre_fterm\", \"pre_since_opened\", \"pre_since_confirmed\", \"pre_till_pclose\", \"pre_till_fclose\",\n",
    "        \"pre_loans5\", \"pre_loans530\", \"pre_loans3060\", \"pre_loans6090\", \"pre_loans90\",\n",
    "        *cfg.cat_cols,  # categorical inputs for OHE\n",
    "    }\n",
    "\n",
    "    # dynamic families\n",
    "    prefixes = [\"enc_paym_\"]\n",
    "\n",
    "    # filter available cols\n",
    "    whitelist = {c for c in needed_exact if c in schema_cols}\n",
    "    for p in prefixes:\n",
    "        whitelist |= {c for c in schema_cols if c.startswith(p)}\n",
    "\n",
    "    READ_COLUMNS = sorted(whitelist)\n",
    "\n",
    "    # save whitelist artifact\n",
    "    WHITELIST_JSON = ARTIFACTS_DIR / \"feature_whitelist.json\"\n",
    "    with open(WHITELIST_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(READ_COLUMNS, f, indent=2)\n",
    "\n",
    "    print(f\"Column whitelist built: {len(READ_COLUMNS)} columns → {WHITELIST_JSON}\")\n",
    "else:\n",
    "    READ_COLUMNS = None\n",
    "    print(\"READ_COLUMNS=None → all columns will be read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f30910e",
   "metadata": {},
   "source": [
    "## 08. Parquet writer setup + options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbcf0c",
   "metadata": {},
   "source": [
    "**Purpose**: Initialize a single-file parquet writer with schema from\n",
    "\n",
    "the first processed batch and set IO performance options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1eeac6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writer state\n",
    "writer = None\n",
    "schema = None\n",
    "written_rows = 0\n",
    "matched_labels = 0\n",
    "batches = 0\n",
    "t0 = time.time()\n",
    "\n",
    "# Writer options\n",
    "WRITE_COMPRESSION = \"snappy\"   # fast & efficient\n",
    "ROW_GROUP_SIZE   = 1_000_000\n",
    "USE_DICTIONARY   = True      # good for strings/low-cardinality ints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca566f",
   "metadata": {},
   "source": [
    "## 09. Sanity: verify whitelist against all shards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b896b",
   "metadata": {},
   "source": [
    "**Purpose**: make sure every shard has every column from READ_COLUMNS; list any missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16f57573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitelist OK across all shards.\n"
     ]
    }
   ],
   "source": [
    "problems = {}\n",
    "for fp in parquet_files:\n",
    "    cols = set(pq.ParquetFile(fp).schema.names)\n",
    "    miss = [c for c in READ_COLUMNS if c not in cols]\n",
    "    if miss:\n",
    "        problems[fp.name] = miss\n",
    "\n",
    "if not problems:\n",
    "    print(\"Whitelist OK across all shards.\")\n",
    "else:\n",
    "    print(\"Missing columns detected:\")\n",
    "    for fn, miss in problems.items():\n",
    "        print(f\" - {fn}: {miss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d970e",
   "metadata": {},
   "source": [
    "## 10. Build last-rn map (light pre-pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ccd68e",
   "metadata": {},
   "source": [
    "**Purpose**: Scan only ['id','rn'] across all shards to compute the latest rn per id (cheap, 2nd pass uses it to filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db372357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[map 5/12] ids so far = 1,250,000\n",
      "[map 10/12] ids so far = 2,500,000\n",
      "Built id→last_rn map for 3,000,000 ids in 23.8s\n"
     ]
    }
   ],
   "source": [
    "id_last_rn: dict[str, int] = {}\n",
    "t0 = time.time()\n",
    "\n",
    "for i, fp in enumerate(parquet_files, 1):\n",
    "    pf = pq.ParquetFile(fp)\n",
    "\n",
    "    # iterate in large row groups to keep IO efficient\n",
    "    for batch in pf.iter_batches(columns=[\"id\", \"rn\"], batch_size=1_000_000):\n",
    "        # convert to pandas; keep ids as strings for stability\n",
    "        try:\n",
    "            b = batch.to_pandas(types_mapper=pd.ArrowDtype)  # pandas >=1.5\n",
    "        except Exception:\n",
    "            b = batch.to_pandas()\n",
    "\n",
    "        # normalize dtypes\n",
    "        b[\"id\"] = b[\"id\"].astype(\"string\")\n",
    "        b[\"rn\"] = pd.to_numeric(b[\"rn\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        # per-batch max rn per id\n",
    "        mx = b.groupby(\"id\", observed=True)[\"rn\"].max()\n",
    "\n",
    "        # merge into global map (keep the largest rn seen so far)\n",
    "        for k, v in mx.items():\n",
    "            if pd.isna(v):\n",
    "                continue\n",
    "            cur = id_last_rn.get(k)\n",
    "            if cur is None or int(v) > cur:\n",
    "                id_last_rn[k] = int(v)\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        print(f\"[map {i}/{len(parquet_files)}] ids so far = {len(id_last_rn):,}\")\n",
    "\n",
    "print(f\"Built id→last_rn map for {len(id_last_rn):,} ids in {time.time() - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59190d7e",
   "metadata": {},
   "source": [
    "## 11. Streaming loop (client-level, with dtype harmonization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e895db7c",
   "metadata": {},
   "source": [
    "**Purpose**: Read whitelisted cols → keep only (id, rn==last_rn[id]) → feature-engineer → join labels → enforce dtypes → write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a44830f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/12] rows=1,250,000 matched=1,250,000\n",
      "[10/12] rows=2,500,000 matched=2,500,000\n",
      "Done (client-level). Rows: 3,000,000 | matched: 3,000,000 | time: 907.0s\n"
     ]
    }
   ],
   "source": [
    "from utils import read_parquet_safe\n",
    "\n",
    "# Ensure whitelist contains critical keys\n",
    "if READ_COLUMNS is not None:\n",
    "    if \"id\" not in READ_COLUMNS: READ_COLUMNS.append(\"id\")\n",
    "    if \"rn\" not in READ_COLUMNS: READ_COLUMNS.append(\"rn\")\n",
    "\n",
    "# Target mapping (nullable to allow missing)\n",
    "target_map = target[\"target\"]            # index is 'id' (string)\n",
    "TARGET_DTYPE = \"Int8\"                    # nullable int (not plain int8)\n",
    "\n",
    "# If a pre-pass already built id_last_rn, reuse it. Otherwise build it quickly here.\n",
    "if \"id_last_rn\" not in globals() or not id_last_rn:\n",
    "    id_last_rn = {}\n",
    "    t0 = time.time()\n",
    "    for i, fp in enumerate(parquet_files, 1):\n",
    "        df_keys = read_parquet_safe(fp, columns=[\"id\", \"rn\"])\n",
    "        ensure_id_column(df_keys, source=getattr(fp, \"name\", str(fp)))\n",
    "        if \"rn\" not in df_keys.columns:\n",
    "            raise KeyError(f\"Missing 'rn' in shard {getattr(fp, 'name', str(fp))}.\")\n",
    "        df_keys[\"id\"] = df_keys[\"id\"].astype(\"string\")\n",
    "        df_keys[\"rn\"] = pd.to_numeric(df_keys[\"rn\"], errors=\"coerce\").astype(\"Int32\")\n",
    "        mx = df_keys.groupby(\"id\", observed=True)[\"rn\"].max()\n",
    "        for k, v in mx.items():\n",
    "            if pd.isna(v): \n",
    "                continue\n",
    "            cur = id_last_rn.get(k)\n",
    "            if cur is None or int(v) > cur:\n",
    "                id_last_rn[k] = int(v)\n",
    "        if i % 5 == 0:\n",
    "            print(f\"[map {i}/{len(parquet_files)}] ids so far = {len(id_last_rn):,}\")\n",
    "    print(f\"Built last-rn map for {len(id_last_rn):,} ids in {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Writer/init state\n",
    "writer = None\n",
    "all_cols: list[str] | None = None\n",
    "written_rows = matched_labels = batches = 0\n",
    "t0 = time.time()\n",
    "\n",
    "# Helper: pick pandas dtype string from DTYPE_RULES\n",
    "def _rule_dtype_of(col: str) -> str:\n",
    "    for dt, cols in DTYPE_RULES.items():\n",
    "        if col in cols:\n",
    "            return {\"Int32\":\"Int32\", \"Int16\":\"Int16\", \"UInt8\":\"UInt8\", \"Float32\":\"float32\"}.get(dt, \"float32\")\n",
    "    return \"float32\"\n",
    "\n",
    "for i, fp in enumerate(parquet_files, 1):\n",
    "    # Read & normalize\n",
    "    df = read_parquet_safe(fp, columns=READ_COLUMNS)\n",
    "    if df.empty:\n",
    "        continue\n",
    "    ensure_id_column(df, source=getattr(fp, \"name\", str(fp)))\n",
    "    if \"rn\" not in df.columns:\n",
    "        raise KeyError(f\"Missing 'rn' in shard {getattr(fp, 'name', str(fp))}.\")\n",
    "    df[\"id\"] = df[\"id\"].astype(\"string\")\n",
    "    df[\"rn\"] = pd.to_numeric(df[\"rn\"], errors=\"coerce\").astype(\"Int32\")\n",
    "\n",
    "    # Keep only last (id, rn)\n",
    "    df[\"_last_rn\"] = df[\"id\"].map(id_last_rn)\n",
    "    keep = df[\"rn\"].eq(df[\"_last_rn\"])\n",
    "    if not keep.any():\n",
    "        continue\n",
    "    df = df.loc[keep].drop(columns=[\"_last_rn\"])\n",
    "\n",
    "    # Tie-break if duplicates with same (id, rn)\n",
    "    if df.duplicated(subset=[\"id\", \"rn\"]).any():\n",
    "        sec = [c for c in [\"pre_since_opened\", \"pre_since_confirmed\", \"pre_fterm\"] if c in df.columns]\n",
    "        df = df.sort_values([\"id\", \"rn\"] + sec).drop_duplicates([\"id\", \"rn\"], keep=\"last\")\n",
    "\n",
    "    # On the first batch, extend dtype rules with actual enc_paym_* present\n",
    "    if all_cols is None:\n",
    "        extend_dtype_rules_with_enc_paym(df.columns)\n",
    "\n",
    "    # Feature engineering (our FG expects (X, paym); pass df twice)\n",
    "    feat = fg.transform(df, df)\n",
    "    feat[\"id\"] = feat[\"id\"].astype(\"string\")\n",
    "\n",
    "    # Join labels (nullable int to allow NaNs for OOS ids)\n",
    "    feat[\"target\"] = feat[\"id\"].map(target_map).astype(TARGET_DTYPE)\n",
    "\n",
    "    # Enforce Arrow-friendly dtypes\n",
    "    feat = harmonize_dtypes(feat, dtype_rules=DTYPE_RULES)\n",
    "\n",
    "    # Union schema across batches (id first)\n",
    "    if all_cols is None:\n",
    "        all_cols = [\"id\"] + [c for c in feat.columns if c != \"id\"]\n",
    "    else:\n",
    "        for c in feat.columns:\n",
    "            if c not in all_cols:\n",
    "                all_cols.append(c)\n",
    "    # Ensure every expected column exists in this batch\n",
    "    for c in all_cols:\n",
    "        if c not in feat.columns:\n",
    "            feat[c] = pd.Series(pd.NA, index=feat.index).astype(_rule_dtype_of(c))\n",
    "\n",
    "    merged = feat[all_cols]\n",
    "\n",
    "    # Write (single-file Parquet with chunked row groups)\n",
    "    table = pa.Table.from_pandas(merged, preserve_index=False)\n",
    "    if writer is None:\n",
    "        if FINAL_DS_PATH.exists():\n",
    "            FINAL_DS_PATH.unlink()\n",
    "        writer = pq.ParquetWriter(\n",
    "            FINAL_DS_PATH,\n",
    "            schema=table.schema,\n",
    "            compression=WRITE_COMPRESSION,\n",
    "            use_dictionary=USE_DICTIONARY,\n",
    "        )\n",
    "\n",
    "    if ROW_GROUP_SIZE and len(merged) > ROW_GROUP_SIZE:\n",
    "        # slice in row-group chunks\n",
    "        for start in range(0, len(merged), ROW_GROUP_SIZE):\n",
    "            writer.write_table(table.slice(start, min(ROW_GROUP_SIZE, len(merged) - start)))\n",
    "    else:\n",
    "        writer.write_table(table)\n",
    "\n",
    "    written_rows   += len(merged)\n",
    "    matched_labels += int(merged[\"target\"].notna().sum())\n",
    "    batches += 1\n",
    "\n",
    "    if batches % 5 == 0:\n",
    "        gc.collect()\n",
    "        print(f\"[{batches}/{len(parquet_files)}] rows={written_rows:,} matched={matched_labels:,}\")\n",
    "\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "\n",
    "print(f\"Done (client-level). Rows: {written_rows:,} | matched: {matched_labels:,} | time: {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d399cdc",
   "metadata": {},
   "source": [
    "## 12.1 Post checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57245065",
   "metadata": {},
   "source": [
    "**Purpose**: Read back a small sample to verify schema; compute key stats for the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca06986",
   "metadata": {},
   "source": [
    "**Note**: `reading the full file may be heavy; sample a few row groups instead.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2653f203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rows_total': 3000000, 'rows_with_target': 3000000, 'missing_target_rate': 0.0}\n"
     ]
    }
   ],
   "source": [
    "sample = pd.read_parquet(FINAL_DS_PATH, engine=\"pyarrow\", columns=[\"id\",\"target\"])\n",
    "n_total = int(sample.shape[0])\n",
    "n_labeled = int(sample[\"target\"].notna().sum())\n",
    "miss_rate = 1.0 - (n_labeled / max(n_total, 1))\n",
    "print({\"rows_total\": n_total, \"rows_with_target\": n_labeled, \"missing_target_rate\": round(miss_rate, 4)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0966f83",
   "metadata": {},
   "source": [
    "## 12.2 Post-checks (extended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96613f4",
   "metadata": {},
   "source": [
    "**Purpose**: Validate schema, duplicates, label coverage, and basic NaN profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a263adf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns: 45\n",
      "{'rows_total': 3000000, 'rows_with_target': 3000000, 'missing_target_rate': 0.0, 'duplicate_ids': 0, 'label_coverage_vs_target_csv': 1.0}\n",
      "Top-10 NaN share:\n",
      " id                        0.0\n",
      "enc_paym_7                0.0\n",
      "enc_paym_9                0.0\n",
      "pre_fterm                 0.0\n",
      "pre_loans3060             0.0\n",
      "pre_loans5                0.0\n",
      "pre_loans530              0.0\n",
      "pre_loans6090             0.0\n",
      "pre_loans90               0.0\n",
      "pre_loans_credit_limit    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pf = pq.ParquetFile(FINAL_DS_PATH)\n",
    "meta_cols = pf.schema_arrow.names\n",
    "print(\"Final columns:\", len(meta_cols))\n",
    "\n",
    "sample = pd.read_parquet(FINAL_DS_PATH, engine=\"pyarrow\")\n",
    "n_total    = int(sample.shape[0])\n",
    "n_labeled  = int(sample[\"target\"].notna().sum()) if \"target\" in sample.columns else 0\n",
    "miss_rate  = 1.0 - (n_labeled / max(n_total, 1))\n",
    "\n",
    "dup_ids = int(sample[\"id\"].duplicated().sum()) if \"id\" in sample.columns else -1\n",
    "label_coverage = n_labeled / max(len(target), 1)\n",
    "\n",
    "nan_share = sample.isna().mean().sort_values(ascending=False).head(10)\n",
    "\n",
    "print({\n",
    "    \"rows_total\": n_total,\n",
    "    \"rows_with_target\": n_labeled,\n",
    "    \"missing_target_rate\": round(miss_rate,4),\n",
    "    \"duplicate_ids\": dup_ids,\n",
    "    \"label_coverage_vs_target_csv\": round(float(label_coverage), 4)\n",
    "})\n",
    "print(\"Top-10 NaN share:\\n\", nan_share)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ab4cc",
   "metadata": {},
   "source": [
    "## 13.1 Report JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa0905",
   "metadata": {},
   "source": [
    "**Purpose**: Persist assembly metrics for reproducibility (counts, timing, file list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3671cc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\final_v2\\credit-risk-management\\reports\\03_final_dataset_report.json\n"
     ]
    }
   ],
   "source": [
    "report: Dict[str, Any] = {\n",
    "    \"n_files\": len(parquet_files),\n",
    "    \"rows_written\": int(written_rows),\n",
    "    \"labels_matched\": int(matched_labels),\n",
    "    \"final_dataset_path\": str(FINAL_DS_PATH),\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"notes\": \"Built by streaming all shards, applying FeatureGenerator and left-joining train_target.csv on 'id'.\",\n",
    "}\n",
    "REPORT_JSON = REPORTS_DIR / \"03_final_dataset_report.json\"\n",
    "with open(REPORT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved:\", REPORT_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e20009",
   "metadata": {},
   "source": [
    "## 13.2 Enrich report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e6d3fc",
   "metadata": {},
   "source": [
    "**Purpose**: Persist schema, file list, label coverage , and FeatureGenerator config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b73ad68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved (enriched): D:\\final_v2\\credit-risk-management\\reports\\03_final_dataset_report.json | schema → D:\\final_v2\\credit-risk-management\\artifacts\\final_dataset_schema.txt\n"
     ]
    }
   ],
   "source": [
    "pf = pq.ParquetFile(FINAL_DS_PATH)\n",
    "meta_cols = pf.schema_arrow.names\n",
    "\n",
    "# label_coverage may be computed in your extended post-check; fall back to None\n",
    "try:\n",
    "    _ = label_coverage\n",
    "except NameError:\n",
    "    label_coverage = None  # will be written as null in JSON\n",
    "\n",
    "report.update({\n",
    "    \"parquet_files\": [str(p) for p in parquet_files],\n",
    "    \"schema_columns\": meta_cols,\n",
    "    \"label_coverage_vs_target_csv\": (float(label_coverage) if label_coverage is not None else None),\n",
    "    \"feature_config\": (getattr(fg, \"cfg\", None).__dict__ if hasattr(fg, \"cfg\") else None),\n",
    "})\n",
    "\n",
    "# also persist schema columns to a txt for quick diffing in git\n",
    "with open(ARTIFACTS_DIR / \"final_dataset_schema.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(meta_cols))\n",
    "\n",
    "# rewrite the report with enriched fields\n",
    "with open(REPORT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved (enriched):\", REPORT_JSON, \"| schema →\", ARTIFACTS_DIR / \"final_dataset_schema.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e9e1c",
   "metadata": {},
   "source": [
    "## 14. Audit: cardinalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a5c969",
   "metadata": {},
   "source": [
    "**Purpose**: Quick sanity checks — total rows vs unique ids and label coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34a32a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_rows': 3000000, 'unique_ids': 3000000, 'rows_per_id_avg': 1.0, 'id_coverage': 1.0}\n"
     ]
    }
   ],
   "source": [
    "pf = pq.ParquetFile(FINAL_DS_PATH)\n",
    "total_rows = pf.metadata.num_rows\n",
    "\n",
    "# Count unique ids by scanning only the 'id' column.\n",
    "unique_ids = set()\n",
    "for rb in pf.iter_batches(columns=[\"id\"], batch_size=1_000_000):\n",
    "    unique_ids.update(rb.column(0).to_pylist())\n",
    "n_ids = len(unique_ids)\n",
    "\n",
    "rows_per_id_avg = total_rows / max(n_ids, 1)\n",
    "y_ids = set(target.index)\n",
    "id_coverage = len(unique_ids & y_ids) / max(n_ids, 1)\n",
    "\n",
    "print({\n",
    "    \"total_rows\": total_rows,\n",
    "    \"unique_ids\": n_ids,\n",
    "    \"rows_per_id_avg\": round(rows_per_id_avg, 3),\n",
    "    \"id_coverage\": round(id_coverage, 4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ed0e6",
   "metadata": {},
   "source": [
    "## 15. Guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87507a09",
   "metadata": {},
   "source": [
    "**Purpose**: Explain next steps for modeling notebooks (04)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d23999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next → 04_modeling_baseline: split train/test (use only rows with target),\n",
      "train baseline models (LogReg/RF/LGBM/XGB), measure ROC-AUC ≥ 0.75. Keep unlabeled rows for later inference if needed.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Next → 04_modeling_baseline: split train/test (use only rows with target),\\n\"\n",
    "    \"train baseline models (LogReg/RF/LGBM/XGB), measure ROC-AUC ≥ 0.75. \"\n",
    "    \"Keep unlabeled rows for later inference if needed.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
