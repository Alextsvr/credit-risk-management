{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6edc7080",
   "metadata": {},
   "source": [
    "## 01. Library import & paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c309de2",
   "metadata": {},
   "source": [
    "**Purpose**: Import libs, set reproducibility, define project paths, and prepare artifacts/reports folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b21a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 2.2.2 | numpy: 1.26.4 | pyarrow: 21.0.0\n",
      "ENV OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Clean logs\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pyarrow\")\n",
    "\n",
    "# Pandas display (debug convenience)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "# Version snapshot (for run metadata)\n",
    "def lib_versions() -> Dict[str, str]:\n",
    "    return {\n",
    "        \"pandas\": pd.__version__,\n",
    "        \"numpy\": np.__version__,\n",
    "        \"pyarrow\": pa.__version__,\n",
    "    }\n",
    "\n",
    "print(f\"pandas: {pd.__version__} | numpy: {np.__version__} | pyarrow: {pa.__version__}\")\n",
    "\n",
    "# Project structure (support running from notebooks/ or project root)\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "SRC_DIR      = (PROJECT_ROOT / \"src\").resolve()\n",
    "DATA_DIR     = (PROJECT_ROOT / \"data\").resolve()\n",
    "ARTIFACTS_DIR= (PROJECT_ROOT / \"artifacts\").resolve()\n",
    "REPORTS_DIR  = (PROJECT_ROOT / \"reports\").resolve()\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure src is importable (for features_extended.py)\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "# Artifacts (v1 & v2)\n",
    "FINAL_DS_V1_PATH = ARTIFACTS_DIR / \"final_dataset.parquet\"        # produced by 03\n",
    "FINAL_DS_V2_PATH = ARTIFACTS_DIR / \"final_dataset_v2.parquet\"     # this notebook output\n",
    "META_V2_JSON     = REPORTS_DIR  / \"final_v2_meta.json\"            # run metadata\n",
    "FEATURE_LIST_V2  = REPORTS_DIR  / \"final_v2_feature_list.json\"    # feature inventory\n",
    "\n",
    "print(\"ENV OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c9a883",
   "metadata": {},
   "source": [
    "## 02. Load base final v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e8c48",
   "metadata": {},
   "source": [
    "**Purpose**: Load the existing final dataset (v1) as a baseline input, apply light downcasting, and set run-specific report paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e81c16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded v1 dataset: (3000000, 45) in 0.4s\n",
      "Downcasted. Columns: 45 | Target: target | IDs: ['id', 'rn']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import import_module\n",
    "\n",
    "# Align report filenames with your existing convention in /reports\n",
    "META_V2_JSON    = REPORTS_DIR / \"03_1_final_dataset_v2_report.json\"   # will be created later\n",
    "FEATURE_LIST_V2 = REPORTS_DIR / \"final_v2_feature_list.json\"          # will be created later\n",
    "\n",
    "# Optional safe reader from src.utils (falls back to pandas if not available)\n",
    "def read_parquet_safe(path: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        m = import_module(\"utils\")\n",
    "        if hasattr(m, \"read_parquet_safe\"):\n",
    "            return m.read_parquet_safe(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "t0 = time.time()\n",
    "df_base = read_parquet_safe(FINAL_DS_V1_PATH)\n",
    "print(f\"Loaded v1 dataset: {df_base.shape} in {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Light numeric downcast to save RAM\n",
    "num_cols = df_base.select_dtypes(include=[\"int64\",\"float64\",\"int32\",\"float32\"]).columns.tolist()\n",
    "for c in num_cols:\n",
    "    if pd.api.types.is_float_dtype(df_base[c]):\n",
    "        df_base[c] = pd.to_numeric(df_base[c], downcast=\"float\")\n",
    "    else:\n",
    "        df_base[c] = pd.to_numeric(df_base[c], downcast=\"integer\")\n",
    "\n",
    "TARGET   = \"target\"\n",
    "ID_COLS  = [\"id\", \"rn\"]\n",
    "\n",
    "print(\"Downcasted. Columns:\", df_base.shape[1], \"| Target:\", TARGET, \"| IDs:\", ID_COLS)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb59950c",
   "metadata": {},
   "source": [
    "## 03. Init FeatureGeneratorExtended (config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7130883",
   "metadata": {},
   "source": [
    "**Purpose**: Configure and initialize the extended feature generator; prepare payment matrix and basic settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b3d6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils.features as f\n",
    "import utils.features_extended as fe\n",
    "importlib.reload(f)\n",
    "importlib.reload(fe)\n",
    "\n",
    "from utils.features_extended import FeatureConfigExtended, FeatureGeneratorExtended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b13f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureConfigExtended FeatureGeneratorExtended\n"
     ]
    }
   ],
   "source": [
    "cfg = FeatureConfigExtended(verbose=True)\n",
    "fg  = FeatureGeneratorExtended(cfg)\n",
    "print(type(cfg).__name__, type(fg).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187bd7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAYM_COLS: 25 | df_base: (3000000, 45) | paym_df: (3000000, 25)\n"
     ]
    }
   ],
   "source": [
    "from utils.features_extended import FeatureConfigExtended, FeatureGeneratorExtended\n",
    "\n",
    "# Detect payment columns (enc_paym_0 is the most recent by project convention)\n",
    "PAYM_COLS = sorted([c for c in df_base.columns if c.startswith(\"enc_paym_\")],\n",
    "                   key=lambda s: int(s.split(\"_\")[-1]))  # enc_paym_0, enc_paym_1, ...\n",
    "\n",
    "paym_df = df_base[PAYM_COLS].copy() if len(PAYM_COLS) else pd.DataFrame(index=df_base.index)\n",
    "\n",
    "# Configure which groups to enable; tweak windows/caps if needed\n",
    "cfg_ext = FeatureConfigExtended(\n",
    "    use_payment_seq=True,\n",
    "    use_ratios=True,\n",
    "    use_bucket_severity=True,\n",
    "    use_interactions=True,\n",
    "    windows=[3, 6, 12, 24],\n",
    "    cap_outliers=True,\n",
    "    cap_bounds={\"_default\": (1, 99)},  # per-column overrides can be added later\n",
    "    eps=1e-4,\n",
    "    # Payment code mapping (adjust if your scheme differs)\n",
    "    paym_ok_values=(0, 1),\n",
    "    paym_late_values=(2, 3, 4, 5, 6, 7, 8, 9),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "fg_ext = FeatureGeneratorExtended(cfg_ext)\n",
    "\n",
    "print(f\"PAYM_COLS: {len(PAYM_COLS)} | df_base: {df_base.shape} | paym_df: {paym_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e11af",
   "metadata": {},
   "source": [
    "## 04. Generate extended features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c777d54",
   "metadata": {},
   "source": [
    "**Purpose**: Build extended features using FeatureGeneratorExtended and assemble v2 dataset (id, rn, features, target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31ef9372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[just] paym_ok_share_3: Share of OK payments within last 3 periods.\n",
      "[just] paym_late_share_3: Share of late payments within last 3 periods.\n",
      "[just] paym_ok_share_6: Share of OK payments within last 6 periods.\n",
      "[just] paym_late_share_6: Share of late payments within last 6 periods.\n",
      "[just] paym_ok_share_12: Share of OK payments within last 12 periods.\n",
      "[just] paym_late_share_12: Share of late payments within last 12 periods.\n",
      "[just] paym_ok_share_24: Share of OK payments within last 24 periods.\n",
      "[just] paym_late_share_24: Share of late payments within last 24 periods.\n",
      "[just] paym_longest_ok_streak_24: Longest consecutive OK streak within ~24 periods.\n",
      "[just] paym_longest_late_streak_24: Longest consecutive LATE streak within ~24 periods.\n",
      "[just] paym_last_late_recency: Recency of the last late event (0=now, 1=prev, NaN=never).\n",
      "[just] paym_last_ok_recency: Recency of the last OK event (0=now, 1=prev, NaN=never).\n",
      "[just] paym_ok_trend_6: Slope of OK share over last 6 periods (trend of discipline).\n",
      "[just] overdue_to_limit: pre_loans_total_overdue normalized by credit limit (capped).\n",
      "[just] maxover_to_limit: pre_loans_max_overdue_sum normalized by credit limit (capped).\n",
      "[just] outstanding_to_limit: pre_loans_outstanding normalized by credit limit (capped).\n",
      "[ratios] pre_loans_next_pay_summ not found; skipping nextpay_to_limit.\n",
      "[just] bucket_severity_score: Weighted sum of delinquency buckets (severity proxy).\n",
      "[just] has_recent_90p: Indicator of 90+ delinquency presence (recent risk proxy).\n",
      "[FeatureGeneratorExtended] Done. Shape: (3000000, 63)\n",
      "Extended features built: +18 columns (total 63) in 180.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "\n",
    "# Keep meta/target\n",
    "ID_COLS = [\"id\", \"rn\"]\n",
    "TARGET  = \"target\"\n",
    "\n",
    "# Separate payment matrix (already prepared as paym_df) and pass the full base frame to the generator\n",
    "X_in  = df_base.copy()\n",
    "X_ext = fg_ext.transform(X_in, paym_df)  # adds extended features on a copy\n",
    "\n",
    "# Ensure target and ids are present and first in order\n",
    "cols_order = ID_COLS + [TARGET] + [c for c in X_ext.columns if c not in ID_COLS + [TARGET]]\n",
    "df_v2 = X_ext[cols_order].copy()\n",
    "\n",
    "print(f\"Extended features built: +{df_v2.shape[1] - df_base.shape[1]} columns \"\n",
    "      f\"(total {df_v2.shape[1]}) in {time.time()-t_start:.1f}s\")\n",
    "\n",
    "# Memory relief\n",
    "del X_in, X_ext\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a5ed5",
   "metadata": {},
   "source": [
    "## 05. Quick post-check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c517e302",
   "metadata": {},
   "source": [
    "**Purpose**: Perform light validation of the v2 dataset — shape, NaN rate, duplicates, class balance, and feature overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82cd558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset v2 shape: (3000000, 63)\n",
      "Total NaNs: 333872 (0.18%)\n",
      "Duplicates by id+rn: 0\n",
      "Target distribution: {0: 0.9645193333333333, 1: 0.03548066666666667}\n",
      "Top 10 NaN-rate columns (%): {'paym_last_ok_recency': 7.76, 'paym_last_late_recency': 3.37, 'id': 0.0, 'paym_late_share_3': 0.0, 'pre_loans_credit_limit': 0.0, 'pre_loans_max_overdue_sum': 0.0, 'pre_loans_outstanding': 0.0, 'pre_loans_total_overdue': 0.0, 'pre_pterm': 0.0, 'pre_since_confirmed': 0.0}\n",
      "Saved: D:\\final_v2\\credit-risk-management\\reports\\03_1_final_dataset_v2_quickcheck.json\n"
     ]
    }
   ],
   "source": [
    "def quick_postcheck(df: pd.DataFrame, target_col: str = \"target\") -> Dict[str, Any]:\n",
    "    \"\"\"Compute quick validation summary for the dataset.\"\"\"\n",
    "    report = {}\n",
    "    report[\"shape\"] = df.shape\n",
    "    report[\"columns\"] = len(df.columns)\n",
    "    report[\"nans_total\"] = int(df.isna().sum().sum())\n",
    "    report[\"nan_rate_pct\"] = float(df.isna().mean().mean() * 100)\n",
    "\n",
    "    # Duplicates by id+rn\n",
    "    if all(col in df.columns for col in [\"id\", \"rn\"]):\n",
    "        dup_count = df.duplicated(subset=[\"id\", \"rn\"]).sum()\n",
    "    else:\n",
    "        dup_count = df.duplicated().sum()\n",
    "    report[\"duplicates\"] = int(dup_count)\n",
    "\n",
    "    # Target distribution\n",
    "    if target_col in df.columns:\n",
    "        vc = df[target_col].value_counts(dropna=False, normalize=True)\n",
    "        report[\"target_dist\"] = {int(k): float(v) for k, v in vc.items()}\n",
    "\n",
    "    # Null-rate per column (top 10)\n",
    "    nulls = df.isna().mean().sort_values(ascending=False).head(10).to_dict()\n",
    "    report[\"top10_null_cols\"] = {k: round(v * 100, 2) for k, v in nulls.items()}\n",
    "\n",
    "    return report\n",
    "\n",
    "qc_report = quick_postcheck(df_v2, target_col=TARGET)\n",
    "\n",
    "print(f\"Dataset v2 shape: {qc_report['shape']}\")\n",
    "print(f\"Total NaNs: {qc_report['nans_total']} ({qc_report['nan_rate_pct']:.2f}%)\")\n",
    "print(f\"Duplicates by id+rn: {qc_report['duplicates']}\")\n",
    "print(\"Target distribution:\", qc_report.get(\"target_dist\", {}))\n",
    "print(\"Top 10 NaN-rate columns (%):\", qc_report[\"top10_null_cols\"])\n",
    "\n",
    "# Save quick summary to report JSON\n",
    "quick_report_path = REPORTS_DIR / \"03_1_final_dataset_v2_quickcheck.json\"\n",
    "with open(quick_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qc_report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved:\", quick_report_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dace146",
   "metadata": {},
   "source": [
    "## 06. Extended checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ef01e3",
   "metadata": {},
   "source": [
    "**Purpose**: Run extended validation — correlation snapshot, identical columns scan, and basic schema metadata (safe & sampled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de82223d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended checks saved → D:\\final_v2\\credit-risk-management\\reports\\03_1_final_dataset_v2_extcheck.json\n",
      "Top 30 corr pairs (abs): [('paym_ok_share_3', 'paym_late_share_3', 1.0), ('paym_ok_share_24', 'paym_late_share_24', 1.0), ('enc_paym_0', 'paym_last_status', 1.0), ('paym_ok_share_12', 'paym_late_share_12', 1.0), ('paym_ok_share_6', 'paym_late_share_6', 1.0)]\n",
      "Identical groups found: 0 (showing up to 20 in report)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "# Config for safe computation on large frames\n",
    "EXT_SAMPLE_ROWS = 200_000   # sample for correlation to avoid OOM\n",
    "TOP_K_CORR_PAIRS = 30       # how many top correlated pairs to keep\n",
    "CORR_THRESHOLD = 0.98       # flag pairs with |corr| >= threshold\n",
    "\n",
    "def sample_df(df: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    if len(df) <= n:\n",
    "        return df\n",
    "    return df.sample(n=n, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "def top_correlated_pairs(df: pd.DataFrame, k: int, thr: float) -> List[Dict[str, Any]]:\n",
    "    num_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if not num_cols:\n",
    "        return []\n",
    "    df_s = sample_df(df[num_cols], EXT_SAMPLE_ROWS)\n",
    "    corr = df_s.corr(numeric_only=True).abs()\n",
    "    # extract upper triangle without diagonal\n",
    "    pairs = []\n",
    "    cols = corr.columns.tolist()\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            c = corr.iat[i, j]\n",
    "            if not np.isnan(c):\n",
    "                pairs.append((cols[i], cols[j], float(c)))\n",
    "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    out = []\n",
    "    for a, b, v in pairs[:k]:\n",
    "        out.append({\"col_a\": a, \"col_b\": b, \"abs_corr\": v, \"flag_high\": v >= thr})\n",
    "    return out\n",
    "\n",
    "def find_identical_columns(df: pd.DataFrame, ignore_cols: List[str]) -> List[List[str]]:\n",
    "    \"\"\"Group columns that are byte-identical (after casting NaN to a sentinel).\"\"\"\n",
    "    cols = [c for c in df.columns if c not in ignore_cols]\n",
    "    sig2cols: Dict[str, List[str]] = {}\n",
    "    for c in cols:\n",
    "        s = df[c]\n",
    "        # normalize to bytes signature\n",
    "        vals = s.fillna(np.nan).to_numpy()\n",
    "        try:\n",
    "            data = vals.tobytes()\n",
    "        except Exception:\n",
    "            # fallback: convert to string (slower but safe)\n",
    "            data = \"|\".join(map(str, s.fillna(\"NaN\").tolist())).encode(\"utf-8\")\n",
    "        key = md5(data).hexdigest()\n",
    "        sig2cols.setdefault(key, []).append(c)\n",
    "    # keep only groups with 2+ members\n",
    "    return [v for v in sig2cols.values() if len(v) >= 2]\n",
    "\n",
    "ext_report: Dict[str, Any] = {}\n",
    "\n",
    "# Basic schema meta\n",
    "ext_report[\"n_rows\"] = int(len(df_v2))\n",
    "ext_report[\"n_cols\"] = int(df_v2.shape[1])\n",
    "ext_report[\"numeric_cols\"] = int(df_v2.select_dtypes(include=[\"number\"]).shape[1])\n",
    "ext_report[\"object_cols\"]  = int(df_v2.select_dtypes(include=[\"object\"]).shape[1])\n",
    "\n",
    "# Correlation snapshot (safe sample)\n",
    "t0 = time.time()\n",
    "ext_report[\"top_corr_pairs\"] = top_correlated_pairs(df_v2.drop(columns=[\"id\",\"rn\",\"target\"], errors=\"ignore\"),\n",
    "                                                    k=TOP_K_CORR_PAIRS, thr=CORR_THRESHOLD)\n",
    "ext_report[\"corr_snapshot_time_sec\"] = round(time.time() - t0, 2)\n",
    "\n",
    "# Identical columns (byte-level)\n",
    "t0 = time.time()\n",
    "ident_groups = find_identical_columns(df_v2.drop(columns=[\"id\",\"rn\",\"target\"], errors=\"ignore\"),\n",
    "                                      ignore_cols=[])\n",
    "ext_report[\"identical_column_groups\"] = ident_groups[:20]  # limit in report\n",
    "ext_report[\"identical_scan_time_sec\"] = round(time.time() - t0, 2)\n",
    "\n",
    "# Save extended report\n",
    "ext_report_path = REPORTS_DIR / \"03_1_final_dataset_v2_extcheck.json\"\n",
    "with open(ext_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ext_report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Extended checks saved → {ext_report_path}\")\n",
    "print(f\"Top {len(ext_report['top_corr_pairs'])} corr pairs (abs):\",\n",
    "      [(p['col_a'], p['col_b'], round(p['abs_corr'],4)) for p in ext_report['top_corr_pairs'][:5]])\n",
    "print(f\"Identical groups found: {len(ident_groups)} (showing up to 20 in report)\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20bceb",
   "metadata": {},
   "source": [
    "## 07. Save dataset v2 (+ meta & feature inventory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9c324",
   "metadata": {},
   "source": [
    "**Purpose**: Apply sentinel imputation, prune redundant features, and persist v2 dataset with metadata & feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf38e636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned 32 redundant features; 31 remain.\n",
      "Saved final_dataset_v2.parquet | (3000000, 31) in 2.49s\n",
      "Saved feature list: D:\\final_v2\\credit-risk-management\\reports\\final_v2_feature_list.json\n",
      "Saved meta report: D:\\final_v2\\credit-risk-management\\reports\\03_1_final_dataset_v2_report.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Sentinel imputation for recency features ---\n",
    "RECENCY_SENTINEL = 25  # since we have enc_paym_0..24 (no event → 25)\n",
    "for col in [\"paym_last_late_recency\", \"paym_last_ok_recency\"]:\n",
    "    if col in df_v2.columns:\n",
    "        df_v2[col] = df_v2[col].fillna(RECENCY_SENTINEL).astype(\"float32\")\n",
    "\n",
    "# --- Feature pruning (to reduce perfect correlation) ---\n",
    "drop_cols = []\n",
    "\n",
    "# Drop paym_ok_share_* (perfectly anticorrelated with paym_late_share_*)\n",
    "drop_cols += [c for c in df_v2.columns if c.startswith(\"paym_ok_share_\")]\n",
    "\n",
    "# Drop redundant streaks and shares except compact subset\n",
    "keep_late_shares = {\"paym_late_share_6\", \"paym_late_share_24\"}\n",
    "keep_streaks     = {\"paym_longest_late_streak_24\"}\n",
    "keep_trend       = {\"paym_ok_trend_6\"}\n",
    "keep_recency     = {\"paym_last_late_recency\"}\n",
    "keep_set = keep_late_shares | keep_streaks | keep_trend | keep_recency\n",
    "\n",
    "drop_cols += [\n",
    "    c for c in df_v2.columns\n",
    "    if c.startswith(\"paym_late_share_\") or\n",
    "       c.startswith(\"paym_longest_\") or\n",
    "       c.startswith(\"paym_ok_trend_\") or\n",
    "       c.startswith(\"paym_last_\")\n",
    "]\n",
    "drop_cols = [c for c in drop_cols if c not in keep_set]\n",
    "\n",
    "# Drop paym_last_status (duplicate of enc_paym_0)\n",
    "if \"paym_last_status\" in df_v2.columns:\n",
    "    drop_cols.append(\"paym_last_status\")\n",
    "\n",
    "# Drop excessive enc_paym_* columns except first three\n",
    "paym_raw_cols = sorted([c for c in df_v2.columns if c.startswith(\"enc_paym_\")],\n",
    "                       key=lambda s: int(s.split(\"_\")[-1]))\n",
    "drop_cols += paym_raw_cols[3:]  # keep enc_paym_0, enc_paym_1, enc_paym_2\n",
    "\n",
    "# Apply drop\n",
    "before_cols = df_v2.shape[1]\n",
    "df_v2.drop(columns=list(set(drop_cols)), inplace=True, errors=\"ignore\")\n",
    "after_cols = df_v2.shape[1]\n",
    "print(f\"Pruned {before_cols - after_cols} redundant features; {after_cols} remain.\")\n",
    "\n",
    "# --- Save dataset v2 ---\n",
    "t0 = time.time()\n",
    "df_v2.to_parquet(FINAL_DS_V2_PATH, index=False)\n",
    "save_time = round(time.time() - t0, 2)\n",
    "print(f\"Saved {FINAL_DS_V2_PATH.name} | {df_v2.shape} in {save_time}s\")\n",
    "\n",
    "# --- Save feature inventory ---\n",
    "feature_list = [c for c in df_v2.columns if c not in [\"id\", \"rn\", \"target\"]]\n",
    "with open(FEATURE_LIST_V2, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sorted(feature_list), f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved feature list: {FEATURE_LIST_V2}\")\n",
    "\n",
    "# --- Save metadata report ---\n",
    "meta = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"seed\": SEED,\n",
    "    \"n_rows\": int(len(df_v2)),\n",
    "    \"n_features\": len(feature_list),\n",
    "    \"save_time_sec\": save_time,\n",
    "    \"removed_features\": sorted(list(set(drop_cols))),\n",
    "    \"recency_sentinel\": RECENCY_SENTINEL,\n",
    "    \"lib_versions\": lib_versions(),\n",
    "}\n",
    "with open(META_V2_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved meta report: {META_V2_JSON}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e8e89",
   "metadata": {},
   "source": [
    "## 08. Changelog: v1 vs v2 feature diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e7953",
   "metadata": {},
   "source": [
    "**Purpose**: Compare feature sets between final_dataset (v1) and final_dataset_v2 (v2), save a concise changelog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "382230f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved changelog → D:\\final_v2\\credit-risk-management\\reports\\final_v2_changelog.json\n",
      "v1→v2: +10 / -24 | common=18\n"
     ]
    }
   ],
   "source": [
    "def load_v1_columns(path: Path) -> List[str]:\n",
    "    df = pd.read_parquet(path, columns=None)\n",
    "    return df.columns.tolist()\n",
    "\n",
    "# Load v1 columns (no heavy ops)\n",
    "v1_cols = load_v1_columns(FINAL_DS_V1_PATH)\n",
    "v2_cols = df_v2.columns.tolist()\n",
    "\n",
    "# Keep only feature columns (exclude meta/target)\n",
    "def feat_only(cols: List[str]) -> List[str]:\n",
    "    return [c for c in cols if c not in (\"id\", \"rn\", \"target\")]\n",
    "\n",
    "v1_feats = set(feat_only(v1_cols))\n",
    "v2_feats = set(feat_only(v2_cols))\n",
    "\n",
    "added    = sorted(v2_feats - v1_feats)\n",
    "removed  = sorted(v1_feats - v2_feats)\n",
    "common   = sorted(v1_feats & v2_feats)\n",
    "\n",
    "changelog = {\n",
    "    \"v1_feature_count\": len(v1_feats),\n",
    "    \"v2_feature_count\": len(v2_feats),\n",
    "    \"added_count\": len(added),\n",
    "    \"removed_count\": len(removed),\n",
    "    \"common_count\": len(common),\n",
    "    \"added\": added[:200],      # cap lists in report\n",
    "    \"removed\": removed[:200],\n",
    "}\n",
    "\n",
    "change_path = REPORTS_DIR / \"final_v2_changelog.json\"\n",
    "with open(change_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(changelog, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved changelog → {change_path}\")\n",
    "print(f\"v1→v2: +{len(added)} / -{len(removed)} | common={len(common)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbc6bb",
   "metadata": {},
   "source": [
    "## 09. Sanity importance (light RandomForest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f42d9",
   "metadata": {},
   "source": [
    "**Purpose**: Train a lightweight RandomForest on a sampled subset to estimate relative feature importances (quick signal check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69541771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF sanity AUC=0.6433 | Saved top-20 importances → D:\\final_v2\\credit-risk-management\\reports\\final_v2_feature_importance_rf_sanity.json\n",
      "enc_paym_2                     0.0952\n",
      "paym_last_late_recency         0.0932\n",
      "enc_paym_0                     0.0868\n",
      "enc_paym_1                     0.0750\n",
      "paym_longest_late_streak_24    0.0584\n",
      "outstanding_to_limit           0.0561\n",
      "paym_late_share_24             0.0551\n",
      "maxover_to_limit               0.0547\n",
      "pre_since_opened               0.0523\n",
      "pre_pterm                      0.0440\n",
      "pre_loans_credit_limit         0.0430\n",
      "pre_loans_max_overdue_sum      0.0402\n",
      "pre_till_fclose                0.0392\n",
      "pre_till_pclose                0.0387\n",
      "pre_fterm                      0.0354\n",
      "paym_late_share_6              0.0347\n",
      "pre_loans_outstanding          0.0298\n",
      "pre_since_confirmed            0.0293\n",
      "paym_ok_trend_6                0.0154\n",
      "bucket_severity_score          0.0125\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Sample subset for quick training (keep stratified balance)\n",
    "SAMPLE_SIZE = 200_000 if len(df_v2) > 200_000 else len(df_v2)\n",
    "df_sample = df_v2.sample(SAMPLE_SIZE, random_state=SEED)\n",
    "X = df_sample.drop(columns=[\"id\", \"rn\", \"target\"], errors=\"ignore\")\n",
    "y = df_sample[\"target\"].astype(int)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "rf_sanity = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=8,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=SEED,\n",
    ")\n",
    "rf_sanity.fit(X_train, y_train)\n",
    "pred = rf_sanity.predict_proba(X_valid)[:, 1]\n",
    "auc = roc_auc_score(y_valid, pred)\n",
    "\n",
    "importances = (\n",
    "    pd.Series(rf_sanity.feature_importances_, index=X.columns)\n",
    "    .sort_values(ascending=False)\n",
    "    .head(20)\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "# Save to report\n",
    "importance_path = REPORTS_DIR / \"final_v2_feature_importance_rf_sanity.json\"\n",
    "with open(importance_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(importances.to_dict(), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"RF sanity AUC={auc:.4f} | Saved top-20 importances → {importance_path}\")\n",
    "print(importances)\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
