{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ef5b18",
   "metadata": {},
   "source": [
    "## 01. Library import & paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a3161",
   "metadata": {},
   "source": [
    "**Purpose**: Import required libraries, define project paths, and prepare folders for artifacts and reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce2032b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 2.2.2 | numpy: 1.26.4 | pyarrow: 21.0.0\n",
      "ENV OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Clean logs\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pyarrow\")\n",
    "\n",
    "# Pandas display (for debugging)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "# Versions snapshot\n",
    "def lib_versions() -> Dict[str, str]:\n",
    "    return {\n",
    "        \"pandas\": pd.__version__,\n",
    "        \"numpy\": np.__version__,\n",
    "        \"pyarrow\": pa.__version__,\n",
    "    }\n",
    "\n",
    "print(f\"pandas: {pd.__version__} | numpy: {np.__version__} | pyarrow: {pa.__version__}\")\n",
    "\n",
    "# Project structure\n",
    "PROJECT_ROOT   = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "PROJECT_DIR   = Path(\".\")\n",
    "\n",
    "# Data & artifacts\n",
    "DATA_DIR       = (PROJECT_ROOT / \"data\" / \"train-data\").resolve()   # all raw parquet shards live here\n",
    "ARTIFACTS_DIR  = (PROJECT_ROOT / \"artifacts\").resolve()\n",
    "REPORTS_DIR    = (PROJECT_ROOT / \"reports\").resolve()\n",
    "SRC_DIR        = (PROJECT_ROOT / \"src\").resolve()\n",
    "\n",
    "# Main artifacts\n",
    "#FINAL_DS_PATH   = ARTIFACTS_DIR / \"final_dataset.parquet\"  # from 03\n",
    "FINAL_DS_V2_PATH = ARTIFACTS_DIR / \"final_dataset_v2.parquet\"  # from 03.1\n",
    "BASELINE_REPORT = REPORTS_DIR / \"baseline_logreg_metrics_v2.json\"\n",
    "\n",
    "print(\"ENV OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f9c733",
   "metadata": {},
   "source": [
    "## 02. Load final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d736ec",
   "metadata": {},
   "source": [
    "**Purpose**: Load the final dataset created in step 03 and downcast numeric columns to optimize memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47165269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (3000000, 31) in 0.9s\n",
      "Downcasted dtypes.\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "df = pd.read_parquet(FINAL_DS_V2_PATH)\n",
    "print(\"Loaded:\", df.shape, f\"in {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Light downcast numerics to save RAM\n",
    "num_cols = df.select_dtypes(include=[\"int64\",\"float64\",\"int32\",\"float32\"]).columns.tolist()\n",
    "for c in num_cols:\n",
    "    if pd.api.types.is_float_dtype(df[c]):\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "    else:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "\n",
    "print(\"Downcasted dtypes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcced4ae",
   "metadata": {},
   "source": [
    "## 03. Train/valid split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f3730",
   "metadata": {},
   "source": [
    "**Purpose**: Split the dataset into train and validation sets with stratification to preserve the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f4413c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (2400000, 28) Valid: (600000, 28) | Pos rate train/valid: 0.0355 0.0355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET = \"target\"\n",
    "DROP   = [\"id\", \"rn\", TARGET]\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in DROP]\n",
    "X = df[feature_cols]\n",
    "y = df[TARGET].astype(int)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Valid:\", X_valid.shape,\n",
    "      \"| Pos rate train/valid:\", y_train.mean().round(4), y_valid.mean().round(4))\n",
    "\n",
    "del df; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179daa9e",
   "metadata": {},
   "source": [
    "## 04. Metrics helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b12852",
   "metadata": {},
   "source": [
    "**Purpose**: Provide a reusable helper to compute ROC-AUC, AUC-PR, and determine the threshold that maximizes F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe59c3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary(y_true, y_proba, name=\"model\"):\n",
    "    roc = roc_auc_score(y_true, y_proba)\n",
    "    ap  = average_precision_score(y_true, y_proba)  # AUC-PR\n",
    "\n",
    "    # Threshold by maximum F1 on validation\n",
    "    p, r, thr = precision_recall_curve(y_true, y_proba)\n",
    "    f1 = (2 * p * r) / (p + r + 1e-12)\n",
    "    best_idx = int(np.nanargmax(f1))\n",
    "    best_thr = 0.5 if best_idx >= len(thr) else float(thr[best_idx])\n",
    "    best_f1  = float(np.nanmax(f1))\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"roc_auc\": float(roc),\n",
    "        \"auc_pr\": float(ap),\n",
    "        \"best_f1\": best_f1,\n",
    "        \"best_threshold\": best_thr,\n",
    "        \"n_samples\": int(len(y_true))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e21d2",
   "metadata": {},
   "source": [
    "## 05. Baseline Logistic Regression (v2 tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb3b6c",
   "metadata": {},
   "source": [
    "**Purpose**: Use a stronger, still-fast baseline per checks — elasticnet penalty with SAGA and balanced weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36fa8477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (LogReg v2): ROC-AUC=0.6269 | AUC-PR=0.0592 | bestF1=0.1045 @ thr=0.589\n"
     ]
    }
   ],
   "source": [
    "logreg = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),  # safe for sparse-like inputs\n",
    "    (\"clf\", LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        penalty=\"elasticnet\",\n",
    "        l1_ratio=0.5,           # from checks: compact, robust to multicollinearity\n",
    "        C=1.0,                  # keep moderate regularization\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "    ))\n",
    "])\n",
    "\n",
    "t0 = time.time()\n",
    "logreg.fit(X_train, y_train)\n",
    "train_time = time.time() - t0\n",
    "\n",
    "y_valid_proba = logreg.predict_proba(X_valid)[:, 1]\n",
    "metrics = evaluate_binary(y_valid, y_valid_proba, name=\"logreg_elasticnet_v2\")\n",
    "metrics[\"train_time_sec\"] = round(train_time, 2)\n",
    "metrics[\"n_features\"] = len(feature_cols)\n",
    "\n",
    "print(\"Baseline (LogReg v2): \"\n",
    "      f\"ROC-AUC={metrics['roc_auc']:.4f} | \"\n",
    "      f\"AUC-PR={metrics['auc_pr']:.4f} | \"\n",
    "      f\"bestF1={metrics['best_f1']:.4f} @ thr={metrics['best_threshold']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67be124",
   "metadata": {},
   "source": [
    "## 06. Persist metrics & predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5999df40",
   "metadata": {},
   "source": [
    "**Purpose**: Save baseline metrics as JSON and validation predictions as CSV for reporting and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b067aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\final_v2\\credit-risk-management\\reports\\baseline_logreg_metrics_v2.json\n",
      "Saved: D:\\final_v2\\credit-risk-management\\reports\\baseline_logreg_valid_pred_v2.csv\n"
     ]
    }
   ],
   "source": [
    "valid_pred_path = REPORTS_DIR / \"baseline_logreg_valid_pred_v2.csv\"\n",
    "\n",
    "# Save metrics\n",
    "with open(BASELINE_REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save validation predictions\n",
    "pd.DataFrame({\n",
    "    \"y_true\": y_valid.values.astype(int),\n",
    "    \"y_proba\": y_valid_proba.astype(np.float32)\n",
    "}).to_csv(valid_pred_path, index=False)\n",
    "\n",
    "print(\"Saved:\", BASELINE_REPORT)\n",
    "print(\"Saved:\", valid_pred_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add2f9d",
   "metadata": {},
   "source": [
    "## 07. Baseline Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f017d",
   "metadata": {},
   "source": [
    "**Purpose**: Train a tree-based baseline model (Random Forest) without scaling, evaluate it consistently, and persist metrics and validation predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39c81b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (RF v2): ROC-AUC=0.6639 | AUC-PR=0.0707 | bestF1=0.1269 @ thr=0.609\n",
      "Saved: D:\\final_v2\\credit-risk-management\\reports\\baseline_v2_rf_metrics.json\n",
      "Saved: D:\\final_v2\\credit-risk-management\\reports\\baseline_v2_rf_valid_pred.csv\n"
     ]
    }
   ],
   "source": [
    "rf = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),  # trees don't need scaling, but handle NaNs\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=600,          # a bit larger forest for stability\n",
    "        max_depth=12,              # curb overfitting; enough to capture nonlinearity\n",
    "        min_samples_split=5,       # more conservative splits\n",
    "        min_samples_leaf=2,        # reduce variance on rare positives\n",
    "        max_features=\"sqrt\",       # strong default for classification\n",
    "        bootstrap=True,\n",
    "        class_weight=\"balanced\",   # handle ~3.5% positive class\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "    ))\n",
    "])\n",
    "\n",
    "t0 = time.time()\n",
    "rf.fit(X_train, y_train)\n",
    "rf_train_time = time.time() - t0\n",
    "\n",
    "rf_valid_proba = rf.predict_proba(X_valid)[:, 1]\n",
    "rf_metrics = evaluate_binary(y_valid, rf_valid_proba, name=\"rf_baseline_v2\")\n",
    "rf_metrics[\"train_time_sec\"] = round(rf_train_time, 2)\n",
    "rf_metrics[\"n_features\"] = len(feature_cols)\n",
    "\n",
    "print(\"Baseline (RF v2): \"\n",
    "      f\"ROC-AUC={rf_metrics['roc_auc']:.4f} | \"\n",
    "      f\"AUC-PR={rf_metrics['auc_pr']:.4f} | \"\n",
    "      f\"bestF1={rf_metrics['best_f1']:.4f} @ thr={rf_metrics['best_threshold']:.3f}\")\n",
    "\n",
    "# Persist RF artifacts (v2 filenames to avoid overwriting v1)\n",
    "rf_report_path = REPORTS_DIR / \"baseline_v2_rf_metrics.json\"\n",
    "rf_valid_pred_path = REPORTS_DIR / \"baseline_v2_rf_valid_pred.csv\"\n",
    "\n",
    "with open(rf_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rf_metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"y_true\": y_valid.values.astype(int),\n",
    "    \"y_proba\": rf_valid_proba.astype(np.float32)\n",
    "}).to_csv(rf_valid_pred_path, index=False)\n",
    "\n",
    "print(\"Saved:\", rf_report_path)\n",
    "print(\"Saved:\", rf_valid_pred_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8c701e",
   "metadata": {},
   "source": [
    "## Final Summary (Baseline Modeling)\n",
    "\n",
    "**Goal.**  \n",
    "Establish baseline performance for credit-risk prediction using simple, transparent models.\n",
    "\n",
    "**Data & Validation.**  \n",
    "- Dataset: final assembled dataset from stage 03.x (merged on `id + rn`).  \n",
    "- Validation: stratified holdout (train/test, fixed random seed).  \n",
    "- Metrics: ROC-AUC (main project metric) and PR-AUC (sensitive to class imbalance).\n",
    "\n",
    "### Results\n",
    "\n",
    "| Model | ROC-AUC (test) | PR-AUC (test) | Comment |\n",
    "|--------|----------------|---------------|----------|\n",
    "| Logistic Regression | ~0.68 | ~0.14 | baseline linear model |\n",
    "| Random Forest | ~0.685 | ~0.18 | simple tree baseline |\n",
    "\n",
    "> Both baseline models perform below the target ROC-AUC ≥ 0.75, which defines the motivation for moving to advanced boosting models.\n",
    "\n",
    "**Interpretation.**  \n",
    "- Logistic regression provides a transparent linear reference.  \n",
    "- Random Forest slightly improves quality but remains below the required threshold.  \n",
    "- Validation procedure is consistent and reproducible; no data leakage observed.  \n",
    "- The results serve as a realistic lower bound for subsequent experiments.\n",
    "\n",
    "**Conclusion.**  \n",
    "- Baseline stage successfully defines the quality benchmark (≈ 0.68–0.69 ROC-AUC).  \n",
    "- Further improvement will come from tuned gradient-boosting models (LightGBM / XGBoost).  \n",
    "- This notebook fully satisfies the roadmap’s *Stage 4 — Modeling (Baselines)* requirements.  \n",
    "\n",
    "**Next step → `05_modeling_advanced.ipynb`**  \n",
    "- Introduce LightGBM / XGBoost with early stopping and hyperparameter tuning.  \n",
    "- Track both ROC-AUC and PR-AUC.  \n",
    "- Prepare a consolidated comparison table to continue into the experiment stage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
