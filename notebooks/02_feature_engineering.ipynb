{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6831f878",
   "metadata": {},
   "source": [
    "## 01. Library import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69bd274",
   "metadata": {},
   "source": [
    "**Purpose**: Import core analytics/visualization stack and record environment versions for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d0a9ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n",
      "pandas: 2.2.2\n",
      "numpy: 1.26.4\n",
      "matplotlib: 3.10.5\n",
      "seaborn: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"matplotlib:\", plt.matplotlib.__version__)\n",
    "print(\"seaborn:\", sns.__version__)\n",
    "\n",
    "\n",
    "plt.rcParams.update({\n",
    "\"figure.figsize\": (8, 4),\n",
    "\"axes.grid\": True,\n",
    "\"axes.titlesize\": 12,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfaa661",
   "metadata": {},
   "source": [
    "## 02. Project paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df21db",
   "metadata": {},
   "source": [
    "**Purpose**: Define project folders and ensure directory structure exists (artifacts/, reports/, src/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ef7f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project folders ready: ..\\artifacts ..\\reports ..\\reports\\..\\figs ..\\src\n"
     ]
    }
   ],
   "source": [
    "PROJECT_DIR = Path(\".\")\n",
    "ARTIFACTS_DIR = PROJECT_DIR / \"../artifacts\"\n",
    "REPORTS_DIR = PROJECT_DIR / \"../reports\"\n",
    "REPORTS_FIGS = REPORTS_DIR / \"../figs\"\n",
    "SRC_DIR = PROJECT_DIR / \"../src\"\n",
    "DATA_DIR = PROJECT_DIR / \"../data/train-data\"\n",
    "TRAIN_TARGET = DATA_DIR / \"train_target.csv\"\n",
    "\n",
    "\n",
    "for p in [ARTIFACTS_DIR, REPORTS_DIR, REPORTS_FIGS, SRC_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"Project folders ready:\", ARTIFACTS_DIR, REPORTS_DIR, REPORTS_FIGS, SRC_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705d7f4",
   "metadata": {},
   "source": [
    "## 03. Trial dataset detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2ea33",
   "metadata": {},
   "source": [
    "**Purpose**: Locate a trial/curated parquet produced in 01 and select the first matching candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb14831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using trial file: ..\\artifacts\\features_trial.parquet\n"
     ]
    }
   ],
   "source": [
    "TRIAL_PARQUET_CANDIDATES = list(ARTIFACTS_DIR.glob(\"*trial*.parquet\")) + \\\n",
    "                            list(ARTIFACTS_DIR.glob(\"*curated*.parquet\"))\n",
    "if not TRIAL_PARQUET_CANDIDATES:\n",
    "    raise FileNotFoundError(\n",
    "        \"No trial parquet found in artifacts/. Expected file like '*trial*.parquet' or '*curated*.parquet'.\"\n",
    "    )\n",
    "TRIAL_PARQUET_PATH = TRIAL_PARQUET_CANDIDATES[0]\n",
    "print(f\"Using trial file: {TRIAL_PARQUET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a37258f",
   "metadata": {},
   "source": [
    "## 04. Import safe parquet reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9075261a",
   "metadata": {},
   "source": [
    "**Purpose**: Import read_parquet_safe from src/utils if available; otherwise provide a simple fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae5635ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported utils.read_parquet_safe\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "try:\n",
    "    from utils import read_parquet_safe as READ_SAFE\n",
    "    print(\"Imported utils.read_parquet_safe\")\n",
    "except Exception as e:\n",
    "    print(\"Warning: cannot import utils.read_parquet_safe:\", e)\n",
    "    import pandas as pd\n",
    "    def READ_SAFE(path, columns=None, sample_n=None, prefer_arrow_dtypes=True):\n",
    "        df = pd.read_parquet(path, columns=columns)\n",
    "        return df.head(sample_n) if sample_n else df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819cabaa",
   "metadata": {},
   "source": [
    "## 06. Load trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee81cc",
   "metadata": {},
   "source": [
    "**Purpose**: Read trial dataframe and show basic shape; start timer for the rest of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a78543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial shape: (1000, 40)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df_trial = READ_SAFE(TRIAL_PARQUET_PATH)\n",
    "print(\"Trial shape:\", df_trial.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84bd3e9",
   "metadata": {},
   "source": [
    "## 07. Generate features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e460a5a",
   "metadata": {},
   "source": [
    "**Purpose**: Use FeatureGenerator to create new features on the trial sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "462de126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1000, 40)\n"
     ]
    }
   ],
   "source": [
    "from utils.features import FeatureGenerator, FeatureConfig\n",
    "\n",
    "fg = FeatureGenerator(FeatureConfig())\n",
    "df_feat = fg.transform(df_trial, df_trial)\n",
    "print(\"Features shape:\", df_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa9371",
   "metadata": {},
   "source": [
    "## 08. Merge labels for trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e12641d",
   "metadata": {},
   "source": [
    "**Purpose**: Attach labels from data/train_target.csv for EDA (correlations). Standardize to 'target'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c34397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels attached: 1000/1000 rows\n",
      "Saved: ..\\artifacts\\features_trial_labeled.parquet\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_TARGET.exists():\n",
    "    y = pd.read_csv(TRAIN_TARGET)\n",
    "    assert \"id\" in y.columns, \"train_target.csv must have 'id'\"\n",
    "    # handle 'flag' vs 'target'\n",
    "    if \"target\" not in y.columns:\n",
    "        assert \"flag\" in y.columns, \"train_target.csv must have either 'target' or 'flag'\"\n",
    "        y = y.rename(columns={\"flag\": \"target\"})\n",
    "    # normalize dtypes and uniqueness\n",
    "    y[\"id\"] = y[\"id\"].astype(\"string\")\n",
    "    y = y.drop_duplicates(\"id\").set_index(\"id\")\n",
    "    y[\"target\"] = y[\"target\"].astype(\"int8\")\n",
    "\n",
    "    # merge into df_feat (keep all rows; labels may be missing)\n",
    "    df_feat = df_feat.copy()\n",
    "    df_feat[\"id\"] = df_feat[\"id\"].astype(\"string\")\n",
    "    before = len(df_feat)\n",
    "    df_feat = df_feat.join(y, on=\"id\", how=\"left\")\n",
    "    matched = int(df_feat[\"target\"].notna().sum())\n",
    "    print(f\"Labels attached: {matched}/{before} rows\")\n",
    "\n",
    "    # optional: save labeled trial for convenience\n",
    "    LABELED_TRIAL = ARTIFACTS_DIR / \"features_trial_labeled.parquet\"\n",
    "    df_feat.to_parquet(LABELED_TRIAL, index=False)\n",
    "    print(\"Saved:\", LABELED_TRIAL)\n",
    "else:\n",
    "    print(\"train_target.csv not found → skipping trial merge (correlation block will be skipped).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a697aa9a",
   "metadata": {},
   "source": [
    "## 08. Report (JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f3b730",
   "metadata": {},
   "source": [
    "**Purpose**: Save a machine-readable report with the list of new features and their justifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db6d7cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report saved → ..\\reports\\02_feature_engineering_report.json\n"
     ]
    }
   ],
   "source": [
    "just = fg.get_justification()\n",
    "report: Dict[str, Any] = {\n",
    "\"n_input_cols\": int(df_trial.shape[1]),\n",
    "\"n_output_cols\": int(df_feat.shape[1]),\n",
    "\"n_new_features\": int(len(just)),\n",
    "\"new_features\": just,\n",
    "}\n",
    "REPORT_JSON_PATH = REPORTS_DIR / \"02_feature_engineering_report.json\"\n",
    "with open(REPORT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "\tjson.dump(report, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Report saved → {REPORT_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d44e1",
   "metadata": {},
   "source": [
    "## 09. Plots: distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be498f",
   "metadata": {},
   "source": [
    "**Purpose**: Produce histograms for key new numeric features and save to reports/figs/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95a9967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2 histograms → ..\\reports\\..\\figs\n"
     ]
    }
   ],
   "source": [
    "key_num_cols = [c for c in [\n",
    "\"debt_to_limit\", \"overdue_to_limit\", \"maxoverdue_to_limit\",\n",
    "\"loan_term_ratio\", \"since_ratio\", \"till_close_gap\",\n",
    "\"total_delays\", \"serious_delay_ratio\",\n",
    "\"paym_good_count\", \"paym_bad_count\", \"paym_last_status\", \"paym_last_clean_streak\",\n",
    "] if c in df_feat.columns]\n",
    "\n",
    "\n",
    "for col in key_num_cols:\n",
    "    plt.figure()\n",
    "    sns.histplot(df_feat[col].dropna(), bins=50, kde=False)\n",
    "    plt.title(f\"Distribution: {col}\")\n",
    "    fig_path = REPORTS_FIGS / f\"hist_{col}.png\"\n",
    "    plt.tight_layout(); plt.savefig(fig_path, dpi=120)\n",
    "    plt.close()\n",
    "    \n",
    "print(f\"Saved {len(key_num_cols)} histograms → {REPORTS_FIGS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5af365",
   "metadata": {},
   "source": [
    "## 10. Plots: correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b4ac8e",
   "metadata": {},
   "source": [
    "**Purpose**: If trial contains 'target', save a correlation heatmap for new features vs target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41cc4b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation heatmap saved.\n"
     ]
    }
   ],
   "source": [
    "TARGET_COL = \"target\"\n",
    "if TARGET_COL in df_feat.columns:\n",
    "    corr_cols = [c for c in key_num_cols + [TARGET_COL] if pd.api.types.is_numeric_dtype(df_feat[c])]\n",
    "    corr = df_feat[corr_cols].corr(numeric_only=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr, annot=False, cmap=\"viridis\")\n",
    "    plt.title(\"Correlation: new features vs target\")\n",
    "    fig_path = REPORTS_FIGS / \"corr_new_features.png\"\n",
    "    plt.tight_layout(); plt.savefig(fig_path, dpi=120)\n",
    "    plt.close()\n",
    "    print(\"Correlation heatmap saved.\")\n",
    "else:\n",
    "    print(\"'target' not found in trial → skipping correlation heatmap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6711d66",
   "metadata": {},
   "source": [
    "## 11. Save artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17b256",
   "metadata": {},
   "source": [
    "**Purspose**: Persist trial dataset with features to artifacts/ as parquet (+ optional CSV preview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc7d1fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ..\\artifacts\\features_trial.parquet\n",
      "Saved CSV preview (≤100k rows): ..\\artifacts\\features_trial.csv\n"
     ]
    }
   ],
   "source": [
    "FEATURES_PARQUET = ARTIFACTS_DIR / \"features_trial.parquet\"\n",
    "FEATURES_CSV = ARTIFACTS_DIR / \"features_trial.csv\"\n",
    "\n",
    "\n",
    "df_feat.to_parquet(FEATURES_PARQUET, index=False)\n",
    "print(f\"Saved: {FEATURES_PARQUET}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    df_feat.head(100_000).to_csv(FEATURES_CSV, index=False)\n",
    "    print(f\"Saved CSV preview (≤100k rows): {FEATURES_CSV}\")\n",
    "except Exception as e:\n",
    "    print(\"CSV save failed (non-critical):\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e04f4f",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73189ba8",
   "metadata": {},
   "source": [
    "**Purpose**: Print execution time and next-step guidance for notebook 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36283b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 6.8 sec.\n",
      "Next: import the same FeatureGenerator in 03_final_dataset and stream over all parquet files.\n"
     ]
    }
   ],
   "source": [
    "elapsed = time.time() - start_time\n",
    "print(f\"Done in {elapsed:.1f} sec.\")\n",
    "print(\"Next: import the same FeatureGenerator in 03_final_dataset and stream over all parquet files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
