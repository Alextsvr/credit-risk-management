{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf9d0d5",
   "metadata": {},
   "source": [
    "## 01. Library import & paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e97f4",
   "metadata": {},
   "source": [
    "**Purpose**: Import libs, set reproducibility, define project paths, and prepare artifacts/reports folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a856415e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 2.2.2 | numpy: 1.26.4 | pyarrow: 21.0.0\n",
      "ENV OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Clean logs\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pyarrow\")\n",
    "\n",
    "# Pandas display (debug convenience)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "# Version snapshot (for run metadata)\n",
    "def lib_versions() -> Dict[str, str]:\n",
    "    return {\n",
    "        \"pandas\": pd.__version__,\n",
    "        \"numpy\": np.__version__,\n",
    "        \"pyarrow\": pa.__version__,\n",
    "    }\n",
    "\n",
    "print(f\"pandas: {pd.__version__} | numpy: {np.__version__} | pyarrow: {pa.__version__}\")\n",
    "\n",
    "# Project structure (support running from notebooks/ or project root)\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "SRC_DIR      = (PROJECT_ROOT / \"src\").resolve()\n",
    "DATA_DIR     = (PROJECT_ROOT / \"data\").resolve()\n",
    "ARTIFACTS_DIR= (PROJECT_ROOT / \"artifacts\").resolve()\n",
    "REPORTS_DIR  = (PROJECT_ROOT / \"reports\").resolve()\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure src is importable (for features_extended.py)\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "# Artifacts (v1 & v3)\n",
    "FINAL_DS_V1_PATH = ARTIFACTS_DIR / \"final_dataset.parquet\"   # produced by 03\n",
    "FINAL_DS_V2_PATH = ARTIFACTS_DIR / \"final_dataset_v2.parquet\" # produced by 03.1\n",
    "FINAL_DS_V3_PATH = ARTIFACTS_DIR / \"final_dataset_v3.parquet\"   # this notebook output\n",
    "META_V3_JSON     = REPORTS_DIR   / \"final_v3_meta.json\"         # run metadata\n",
    "FEATURE_LIST_V3  = REPORTS_DIR   / \"final_v3_feature_list.json\" # feature inventory\n",
    "\n",
    "print(\"ENV OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab3e38",
   "metadata": {},
   "source": [
    "## 02. Load base final v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6321a",
   "metadata": {},
   "source": [
    "**Purpose**: Load the existing final dataset (v1) as a baseline input, apply light downcasting, and set run-specific report paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dba3a3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded v1 dataset: (3000000, 45) in 0.4s\n",
      "Downcasted. Columns: 45 | Target: target | IDs: ['id', 'rn']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2016"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import import_module\n",
    "\n",
    "# Align report filenames with your existing convention in /reports\n",
    "META_V2_JSON    = REPORTS_DIR / \"03_1_final_dataset_v2_report.json\"   # will be created later\n",
    "FEATURE_LIST_V2 = REPORTS_DIR / \"final_v2_feature_list.json\"          # will be created later\n",
    "\n",
    "# Optional safe reader from src.utils (falls back to pandas if not available)\n",
    "def read_parquet_safe(path: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        m = import_module(\"utils\")\n",
    "        if hasattr(m, \"read_parquet_safe\"):\n",
    "            return m.read_parquet_safe(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "t0 = time.time()\n",
    "df_base = read_parquet_safe(FINAL_DS_V1_PATH)\n",
    "print(f\"Loaded v1 dataset: {df_base.shape} in {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Light numeric downcast to save RAM\n",
    "num_cols = df_base.select_dtypes(include=[\"int64\",\"float64\",\"int32\",\"float32\"]).columns.tolist()\n",
    "for c in num_cols:\n",
    "    if pd.api.types.is_float_dtype(df_base[c]):\n",
    "        df_base[c] = pd.to_numeric(df_base[c], downcast=\"float\")\n",
    "    else:\n",
    "        df_base[c] = pd.to_numeric(df_base[c], downcast=\"integer\")\n",
    "\n",
    "TARGET   = \"target\"\n",
    "ID_COLS  = [\"id\", \"rn\"]\n",
    "\n",
    "print(\"Downcasted. Columns:\", df_base.shape[1], \"| Target:\", TARGET, \"| IDs:\", ID_COLS)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45ba1f9",
   "metadata": {},
   "source": [
    "## 03. Init FeatureGeneratorExtended (config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df11b06",
   "metadata": {},
   "source": [
    "**Purpose**: Configure and initialize the extended feature generator; prepare payment matrix and basic settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7197b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils.features as f\n",
    "import utils.features_extended as fe\n",
    "importlib.reload(f)\n",
    "importlib.reload(fe)\n",
    "\n",
    "from utils.features_extended import FeatureConfigExtended, FeatureGeneratorExtended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af3b9074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureConfigExtended FeatureGeneratorExtended\n"
     ]
    }
   ],
   "source": [
    "cfg = FeatureConfigExtended(verbose=True)\n",
    "fg  = FeatureGeneratorExtended(cfg)\n",
    "print(type(cfg).__name__, type(fg).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd2320df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAYM_COLS: 25 | df_base: (3000000, 45) | paym_df: (3000000, 25)\n"
     ]
    }
   ],
   "source": [
    "from utils.features_extended import FeatureConfigExtended, FeatureGeneratorExtended\n",
    "\n",
    "# Detect payment columns (enc_paym_0 is the most recent by project convention)\n",
    "PAYM_COLS = sorted([c for c in df_base.columns if c.startswith(\"enc_paym_\")],\n",
    "                   key=lambda s: int(s.split(\"_\")[-1]))  # enc_paym_0, enc_paym_1, ...\n",
    "\n",
    "paym_df = df_base[PAYM_COLS].copy() if len(PAYM_COLS) else pd.DataFrame(index=df_base.index)\n",
    "\n",
    "# Configure which groups to enable; tweak windows/caps if needed\n",
    "cfg_ext = FeatureConfigExtended(\n",
    "    use_payment_seq=True,\n",
    "    use_ratios=True,\n",
    "    use_bucket_severity=True,\n",
    "    use_interactions=True,\n",
    "    windows=[3, 6, 12, 24],\n",
    "    cap_outliers=True,\n",
    "    cap_bounds={\"_default\": (1, 99)},  # per-column overrides can be added later\n",
    "    eps=1e-4,\n",
    "    # Payment code mapping (adjust if your scheme differs)\n",
    "    paym_ok_values=(0, 1),\n",
    "    paym_late_values=(2, 3, 4, 5, 6, 7, 8, 9),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "fg_ext = FeatureGeneratorExtended(cfg_ext)\n",
    "\n",
    "print(f\"PAYM_COLS: {len(PAYM_COLS)} | df_base: {df_base.shape} | paym_df: {paym_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a626c272",
   "metadata": {},
   "source": [
    "## 04. Generate extended features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfcf3b2",
   "metadata": {},
   "source": [
    "**Purpose**: Build extended features using FeatureGeneratorExtended and assemble v2 dataset (id, rn, features, target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6df4398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[just] paym_ok_share_3: Share of OK payments within last 3 periods.\n",
      "[just] paym_late_share_3: Share of late payments within last 3 periods.\n",
      "[just] paym_ok_share_6: Share of OK payments within last 6 periods.\n",
      "[just] paym_late_share_6: Share of late payments within last 6 periods.\n",
      "[just] paym_ok_share_12: Share of OK payments within last 12 periods.\n",
      "[just] paym_late_share_12: Share of late payments within last 12 periods.\n",
      "[just] paym_ok_share_24: Share of OK payments within last 24 periods.\n",
      "[just] paym_late_share_24: Share of late payments within last 24 periods.\n",
      "[just] paym_longest_ok_streak_24: Longest consecutive OK streak within ~24 periods.\n",
      "[just] paym_longest_late_streak_24: Longest consecutive LATE streak within ~24 periods.\n",
      "[just] paym_last_late_recency: Recency of the last late event (0=now, 1=prev, NaN=never).\n",
      "[just] paym_last_ok_recency: Recency of the last OK event (0=now, 1=prev, NaN=never).\n",
      "[just] paym_ok_trend_6: Slope of OK share over last 6 periods (trend of discipline).\n",
      "[just] overdue_to_limit: pre_loans_total_overdue normalized by credit limit (capped).\n",
      "[just] maxover_to_limit: pre_loans_max_overdue_sum normalized by credit limit (capped).\n",
      "[just] outstanding_to_limit: pre_loans_outstanding normalized by credit limit (capped).\n",
      "[ratios] pre_loans_next_pay_summ not found; skipping nextpay_to_limit.\n",
      "[just] bucket_severity_score: Weighted sum of delinquency buckets (severity proxy).\n",
      "[just] has_recent_90p: Indicator of 90+ delinquency presence (recent risk proxy).\n",
      "[FeatureGeneratorExtended] Done. Shape: (3000000, 63)\n",
      "Extended features built: +18 columns (total 63) in 164.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "\n",
    "# Keep meta/target\n",
    "ID_COLS = [\"id\", \"rn\"]\n",
    "TARGET  = \"target\"\n",
    "\n",
    "# Separate payment matrix (already prepared as paym_df) and pass the full base frame to the generator\n",
    "X_in  = df_base.copy()\n",
    "X_ext = fg_ext.transform(X_in, paym_df)  # adds extended features on a copy\n",
    "\n",
    "# Ensure target and ids are present and first in order\n",
    "cols_order = ID_COLS + [TARGET] + [c for c in X_ext.columns if c not in ID_COLS + [TARGET]]\n",
    "df_v2 = X_ext[cols_order].copy()\n",
    "\n",
    "print(f\"Extended features built: +{df_v2.shape[1] - df_base.shape[1]} columns \"\n",
    "      f\"(total {df_v2.shape[1]}) in {time.time()-t_start:.1f}s\")\n",
    "\n",
    "# Memory relief\n",
    "del X_in, X_ext\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce2773",
   "metadata": {},
   "source": [
    "## 05. Init FeatureGeneratorExtendedV2 (config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65909e2",
   "metadata": {},
   "source": [
    "**Purpose**: Configure and initialize the extended feature generator v2; prepare payment matrix and basic settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e18483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils.features as f\n",
    "import utils.features_extended_v2 as fe\n",
    "\n",
    "importlib.reload(f)\n",
    "importlib.reload(fe)\n",
    "\n",
    "from utils.features_extended_v2 import (\n",
    "    FeatureConfigExtendedV2,\n",
    "    FeatureGeneratorExtendedV2,\n",
    "    sanitize_dtypes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3df7f723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureConfigExtendedV2 FeatureGeneratorExtendedV2\n"
     ]
    }
   ],
   "source": [
    "cfg = FeatureConfigExtendedV2(verbose=True)\n",
    "fg  = FeatureGeneratorExtendedV2(cfg)\n",
    "print(type(cfg).__name__, type(fg).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba55dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAYM_COLS: 25  |  df_v2: (3000000, 63)  |  paym_df: (3000000, 25)\n"
     ]
    }
   ],
   "source": [
    "# Sanitize dtypes once (pyarrow/bool → numpy)\n",
    "df_v2 = sanitize_dtypes(df_v2)\n",
    "\n",
    "# Detect payment columns (enc_paym_0 is the most recent by project convention)\n",
    "PAYM_COLS = sorted(\n",
    "    [c for c in df_v2.columns if c.startswith(\"enc_paym_\")],\n",
    "    key=lambda s: int(s.split(\"_\")[-1]) if s.split(\"_\")[-1].isdigit() else 10**9\n",
    ")  # enc_paym_0, enc_paym_1, ...\n",
    "\n",
    "paym_df = df_v2[PAYM_COLS].copy() if len(PAYM_COLS) else pd.DataFrame(index=df_v2.index)\n",
    "\n",
    "# Configure which groups to enable; tweak windows/caps if needed\n",
    "cfg_ext = FeatureConfigExtendedV2(\n",
    "    # базовые\n",
    "    use_payment_seq=True,\n",
    "    use_payment_transitions=True,\n",
    "    use_ratios=True,\n",
    "    use_outstanding_ratios=True,\n",
    "    use_bucket_severity=True,\n",
    "    use_interactions=True,\n",
    "    use_zero_aggregates=True,\n",
    "    use_logs=True,\n",
    "    use_time_decay=True,\n",
    "\n",
    "    windows=[3, 6, 12, 24],\n",
    "    cap_outliers=True,\n",
    "    cap_bounds={\"_default\": (1, 99)},\n",
    "    eps=1e-4,\n",
    "\n",
    "    # МНОВЕДЁМ НОВОЕ:\n",
    "    use_momentum=True,\n",
    "    use_cross_ratios=True,\n",
    "    use_behavioral_flags=True,\n",
    "    use_age_exposure=True,\n",
    "    use_interaction_grid=True,\n",
    "\n",
    "    # multi-decay (добавит несколько версий decay-скора)\n",
    "    time_decay=0.88,               # legacy-скаляр (оставляем)\n",
    "    time_decays=[0.88, 0.95, 0.70],# новые множественные\n",
    "\n",
    "    # momentum окна\n",
    "    momentum_windows=[6, 12, 24],\n",
    "\n",
    "    # хотим дискретизацию util? ставим >1 (0 — выкл)\n",
    "    risk_band_bins=10,              # например, 10 чтобы включить квантильные бины\n",
    "\n",
    "    # карты статусов (оставь как есть)\n",
    "    paym_ok_values=(0, 1),\n",
    "    paym_late_values=(2, 3, 4, 5, 6, 7, 8, 9),\n",
    "\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "fg_ext = FeatureGeneratorExtendedV2(cfg_ext)\n",
    "\n",
    "print(f\"PAYM_COLS: {len(PAYM_COLS)}  |  df_v2: {df_v2.shape}  |  paym_df: {paym_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcb77451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'rn', 'target', 'enc_paym_0', 'enc_paym_1', 'enc_paym_10', 'enc_paym_11', 'enc_paym_12', 'enc_paym_13',\n",
       "       'enc_paym_14', 'enc_paym_15', 'enc_paym_16', 'enc_paym_17', 'enc_paym_18', 'enc_paym_19', 'enc_paym_2',\n",
       "       'enc_paym_20', 'enc_paym_21', 'enc_paym_22', 'enc_paym_23', 'enc_paym_24', 'enc_paym_3', 'enc_paym_4',\n",
       "       'enc_paym_5', 'enc_paym_6', 'enc_paym_7', 'enc_paym_8', 'enc_paym_9', 'pre_fterm', 'pre_loans3060',\n",
       "       'pre_loans5', 'pre_loans530', 'pre_loans6090', 'pre_loans90', 'pre_loans_credit_limit',\n",
       "       'pre_loans_max_overdue_sum', 'pre_loans_outstanding', 'pre_loans_total_overdue', 'pre_pterm',\n",
       "       'pre_since_confirmed', 'pre_since_opened', 'pre_till_fclose', 'pre_till_pclose', 'paym_last_status',\n",
       "       'paym_last_clean_streak', 'paym_ok_share_3', 'paym_late_share_3', 'paym_ok_share_6', 'paym_late_share_6',\n",
       "       'paym_ok_share_12', 'paym_late_share_12', 'paym_ok_share_24', 'paym_late_share_24', 'paym_longest_ok_streak_24',\n",
       "       'paym_longest_late_streak_24', 'paym_last_late_recency', 'paym_last_ok_recency', 'paym_ok_trend_6',\n",
       "       'overdue_to_limit', 'maxover_to_limit', 'outstanding_to_limit', 'bucket_severity_score', 'has_recent_90p'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba3b760f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['enc_paym_0', 'enc_paym_1', 'enc_paym_2', 'enc_paym_3', 'enc_paym_4', 'enc_paym_5', 'enc_paym_6', 'enc_paym_7',\n",
       "       'enc_paym_8', 'enc_paym_9', 'enc_paym_10', 'enc_paym_11', 'enc_paym_12', 'enc_paym_13', 'enc_paym_14',\n",
       "       'enc_paym_15', 'enc_paym_16', 'enc_paym_17', 'enc_paym_18', 'enc_paym_19', 'enc_paym_20', 'enc_paym_21',\n",
       "       'enc_paym_22', 'enc_paym_23', 'enc_paym_24'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paym_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d76e9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'enc_paym_0', 'enc_paym_1', 'enc_paym_10', 'enc_paym_11', 'enc_paym_12', 'enc_paym_13', 'enc_paym_14',\n",
       "       'enc_paym_15', 'enc_paym_16', 'enc_paym_17', 'enc_paym_18', 'enc_paym_19', 'enc_paym_2', 'enc_paym_20',\n",
       "       'enc_paym_21', 'enc_paym_22', 'enc_paym_23', 'enc_paym_24', 'enc_paym_3', 'enc_paym_4', 'enc_paym_5',\n",
       "       'enc_paym_6', 'enc_paym_7', 'enc_paym_8', 'enc_paym_9', 'pre_fterm', 'pre_loans3060', 'pre_loans5',\n",
       "       'pre_loans530', 'pre_loans6090', 'pre_loans90', 'pre_loans_credit_limit', 'pre_loans_max_overdue_sum',\n",
       "       'pre_loans_outstanding', 'pre_loans_total_overdue', 'pre_pterm', 'pre_since_confirmed', 'pre_since_opened',\n",
       "       'pre_till_fclose', 'pre_till_pclose', 'rn', 'paym_last_status', 'paym_last_clean_streak', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c62dca",
   "metadata": {},
   "source": [
    "## 06. Generate extended features V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675e7ab",
   "metadata": {},
   "source": [
    "**Purpose**: Apply the new `FeatureGeneratorExtendedV2` to the enriched dataset (`df_v2`), build second-level features (ratios, transitions, logs, time-decay, interactions), and assemble the final dataset version `v3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93fb5e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[just] paym_ok_share_3: Share of OK payments within last 3 periods.\n",
      "[just] paym_late_share_3: Share of late payments within last 3 periods.\n",
      "[just] paym_ok_share_6: Share of OK payments within last 6 periods.\n",
      "[just] paym_late_share_6: Share of late payments within last 6 periods.\n",
      "[just] paym_ok_share_12: Share of OK payments within last 12 periods.\n",
      "[just] paym_late_share_12: Share of late payments within last 12 periods.\n",
      "[just] paym_ok_share_24: Share of OK payments within last 24 periods.\n",
      "[just] paym_late_share_24: Share of late payments within last 24 periods.\n",
      "[just] paym_longest_ok_streak_24: Longest consecutive OK streak within ~24 periods.\n",
      "[just] paym_longest_late_streak_24: Longest consecutive LATE streak within ~24 periods.\n",
      "[just] paym_last_late_recency: Recency of the last late event (0=now, 1=prev, NaN=never).\n",
      "[just] paym_last_ok_recency: Recency of the last OK event (0=now, 1=prev, NaN=never).\n",
      "[just] paym_ok_trend_6: Slope of OK share over last 6 periods.\n",
      "[just] paym_transitions_3: Number of good↔bad switches over last 3 periods.\n",
      "[just] paym_late_std_3: Volatility of late indicator over last 3 periods.\n",
      "[just] paym_any_late_3: Any late present within last 3 periods.\n",
      "[just] paym_transitions_6: Number of good↔bad switches over last 6 periods.\n",
      "[just] paym_late_std_6: Volatility of late indicator over last 6 periods.\n",
      "[just] paym_any_late_6: Any late present within last 6 periods.\n",
      "[just] paym_transitions_12: Number of good↔bad switches over last 12 periods.\n",
      "[just] paym_late_std_12: Volatility of late indicator over last 12 periods.\n",
      "[just] paym_any_late_12: Any late present within last 12 periods.\n",
      "[just] paym_transitions_24: Number of good↔bad switches over last 24 periods.\n",
      "[just] paym_late_std_24: Volatility of late indicator over last 24 periods.\n",
      "[just] paym_any_late_24: Any late present within last 24 periods.\n",
      "[just] paym_late_time_decay: Recency-weighted sum of late events (decay=0.88).\n",
      "[just] paym_ok_share_delta_3_12: Delta of OK share: paym_ok_share_3 - paym_ok_share_12 (recent improvement/deterioration).\n",
      "[just] paym_ok_share_delta_6_12: Delta of OK share: paym_ok_share_6 - paym_ok_share_12 (recent improvement/deterioration).\n",
      "[just] overdue_to_limit: pre_loans_total_overdue normalized by credit limit (winsorized).\n",
      "[just] maxover_to_limit: pre_loans_max_overdue_sum normalized by credit limit (winsorized).\n",
      "[just] outstanding_to_limit: pre_loans_outstanding normalized by credit limit (winsorized).\n",
      "[V2/ratios_to_limit] pre_loans_next_pay_summ not found; skipping nextpay_to_limit.\n",
      "[just] overdue_ratio: Total overdue normalized by outstanding balance.\n",
      "[just] bucket_severity_score: Weighted sum of delinquency buckets (severity proxy).\n",
      "[just] has_recent_90p: Indicator of 90+ delinquency presence (recent risk proxy).\n",
      "[just] util_x_overdue: Interaction: utilization (or proxy) × overdue_to_limit.\n",
      "[just] util_x_outstanding: Interaction: utilization (or proxy) × outstanding_to_limit.\n",
      "[just] util_log1p: log1p of utilization (or proxy) to stabilize tails.\n",
      "[just] outstanding_log1p: log1p transform of pre_loans_outstanding (stabilizes heavy tails).\n",
      "[just] total_overdue_log1p: log1p transform of pre_loans_total_overdue (stabilizes heavy tails).\n",
      "[just] paym_late_momentum_6: Momentum of late share: share_6 - share_3.\n",
      "[just] paym_late_momentum_12: Momentum of late share: share_12 - share_6.\n",
      "[just] paym_late_momentum_24: Momentum of late share: share_24 - share_12.\n",
      "[just] paym_late_time_decay_088: Recency-weighted late sum with decay=0.88.\n",
      "[just] paym_late_time_decay_095: Recency-weighted late sum with decay=0.95.\n",
      "[just] paym_late_time_decay_07: Recency-weighted late sum with decay=0.7.\n",
      "[just] overdue_to_outstanding: Total overdue normalized by outstanding.\n",
      "[just] maxover_to_outstanding: Max overdue sum normalized by outstanding.\n",
      "[just] credit_headroom: 1 - outstanding_to_limit (remaining headroom).\n",
      "[just] flag_recently_late: 1 if last late recency < 3.\n",
      "[just] flag_high_util_recently: High utilization & recent late share > 0.2.\n",
      "[just] flag_clean_but_high_util: No late in 24 but utilization > 0.8.\n",
      "[just] age_since_opened_minus_confirmed: Difference between since_opened and since_confirmed.\n",
      "[just] recency_x_total_overdue: Interaction: last late recency × total overdue.\n",
      "[just] late6_x_util: Interaction: late share 6 × outstanding_to_limit.\n",
      "[just] util_band_q10: Quantile band of utilization proxy (q=10).\n",
      "[FeatureGeneratorExtendedV2] Done. Shape: (3000000, 100)\n",
      "V2 → V3: +37 columns (total 100) in 30.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build extended features (V2) using upgraded generator with proxy utilities\n",
    "t_start = time.time()\n",
    "\n",
    "# --- IDs & target ---\n",
    "ID_COLS = [\"id\", \"rn\"]\n",
    "TARGET  = \"target\"\n",
    "\n",
    "# --- Transform df_v2 with FeatureGeneratorExtendedV2 ---\n",
    "X_in  = df_v2.copy()\n",
    "X_ext = fg_ext.transform(X_in, paym_df)   # adds new V2 features\n",
    "\n",
    "# --- Ensure proper column order ---\n",
    "cols_order = ID_COLS + [TARGET] + [c for c in X_ext.columns if c not in ID_COLS + [TARGET]]\n",
    "df_v3 = X_ext[cols_order].copy()\n",
    "\n",
    "print(f\"V2 → V3: +{df_v3.shape[1] - df_v2.shape[1]} columns \"\n",
    "      f\"(total {df_v3.shape[1]}) in {time.time() - t_start:.1f}s\")\n",
    "\n",
    "# --- Memory cleanup ---\n",
    "del X_in, X_ext\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d3162d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'rn', 'target', 'enc_paym_0', 'enc_paym_1', 'enc_paym_10', 'enc_paym_11', 'enc_paym_12', 'enc_paym_13',\n",
       "       'enc_paym_14', 'enc_paym_15', 'enc_paym_16', 'enc_paym_17', 'enc_paym_18', 'enc_paym_19', 'enc_paym_2',\n",
       "       'enc_paym_20', 'enc_paym_21', 'enc_paym_22', 'enc_paym_23', 'enc_paym_24', 'enc_paym_3', 'enc_paym_4',\n",
       "       'enc_paym_5', 'enc_paym_6', 'enc_paym_7', 'enc_paym_8', 'enc_paym_9', 'pre_fterm', 'pre_loans3060',\n",
       "       'pre_loans5', 'pre_loans530', 'pre_loans6090', 'pre_loans90', 'pre_loans_credit_limit',\n",
       "       'pre_loans_max_overdue_sum', 'pre_loans_outstanding', 'pre_loans_total_overdue', 'pre_pterm',\n",
       "       'pre_since_confirmed', 'pre_since_opened', 'pre_till_fclose', 'pre_till_pclose', 'paym_last_status',\n",
       "       'paym_last_clean_streak', 'paym_ok_share_3', 'paym_late_share_3', 'paym_ok_share_6', 'paym_late_share_6',\n",
       "       'paym_ok_share_12', 'paym_late_share_12', 'paym_ok_share_24', 'paym_late_share_24', 'paym_longest_ok_streak_24',\n",
       "       'paym_longest_late_streak_24', 'paym_last_late_recency', 'paym_last_ok_recency', 'paym_ok_trend_6',\n",
       "       'overdue_to_limit', 'maxover_to_limit', 'outstanding_to_limit', 'bucket_severity_score', 'has_recent_90p',\n",
       "       'paym_transitions_3', 'paym_late_std_3', 'paym_any_late_3', 'paym_transitions_6', 'paym_late_std_6',\n",
       "       'paym_any_late_6', 'paym_transitions_12', 'paym_late_std_12', 'paym_any_late_12', 'paym_transitions_24',\n",
       "       'paym_late_std_24', 'paym_any_late_24', 'paym_late_time_decay', 'paym_ok_share_delta_3_12',\n",
       "       'paym_ok_share_delta_6_12', 'overdue_ratio', 'util_x_overdue', 'util_x_outstanding', 'util_log1p',\n",
       "       'outstanding_log1p', 'total_overdue_log1p', 'paym_late_momentum_6', 'paym_late_momentum_12',\n",
       "       'paym_late_momentum_24', 'paym_late_time_decay_088', 'paym_late_time_decay_095', 'paym_late_time_decay_07',\n",
       "       'overdue_to_outstanding', 'maxover_to_outstanding', 'credit_headroom', 'flag_recently_late',\n",
       "       'flag_high_util_recently', 'flag_clean_but_high_util', 'age_since_opened_minus_confirmed',\n",
       "       'recency_x_total_overdue', 'late6_x_util'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b93d9b5",
   "metadata": {},
   "source": [
    "## 07. Quick post-check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba04af",
   "metadata": {},
   "source": [
    "**Purpose**: Perform light validation of the v3 dataset — shape, NaN rate, duplicates, class balance, and feature overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d6dbefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset v3 shape: (3000000, 100)\n",
      "Total NaNs: 434943 (0.14%)\n",
      "Duplicates by id+rn: 0\n",
      "Target distribution: {0: 0.9645193333333333, 1: 0.03548066666666667}\n",
      "Top 10 NaN-rate columns (%): {'paym_last_ok_recency': 7.76, 'recency_x_total_overdue': 3.37, 'paym_last_late_recency': 3.37, 'id': 0.0, 'paym_transitions_3': 0.0, 'paym_transitions_24': 0.0, 'paym_any_late_12': 0.0, 'paym_late_std_12': 0.0, 'paym_transitions_12': 0.0, 'paym_any_late_6': 0.0}\n",
      "Saved: D:\\final_v2\\credit-risk-management\\reports\\03_2_final_dataset_v3_quickcheck.json\n"
     ]
    }
   ],
   "source": [
    "def quick_postcheck(df: pd.DataFrame, target_col: str = \"target\") -> Dict[str, Any]:\n",
    "    \"\"\"Compute quick validation summary for the dataset.\"\"\"\n",
    "    report = {}\n",
    "    report[\"shape\"] = df.shape\n",
    "    report[\"columns\"] = len(df.columns)\n",
    "    report[\"nans_total\"] = int(df.isna().sum().sum())\n",
    "    report[\"nan_rate_pct\"] = float(df.isna().mean().mean() * 100)\n",
    "\n",
    "    # Duplicates by id+rn\n",
    "    if all(col in df.columns for col in [\"id\", \"rn\"]):\n",
    "        dup_count = df.duplicated(subset=[\"id\", \"rn\"]).sum()\n",
    "    else:\n",
    "        dup_count = df.duplicated().sum()\n",
    "    report[\"duplicates\"] = int(dup_count)\n",
    "\n",
    "    # Target distribution\n",
    "    if target_col in df.columns:\n",
    "        vc = df[target_col].value_counts(dropna=False, normalize=True)\n",
    "        report[\"target_dist\"] = {int(k): float(v) for k, v in vc.items()}\n",
    "\n",
    "    # Null-rate per column (top 10)\n",
    "    nulls = df.isna().mean().sort_values(ascending=False).head(10).to_dict()\n",
    "    report[\"top10_null_cols\"] = {k: round(v * 100, 2) for k, v in nulls.items()}\n",
    "\n",
    "    return report\n",
    "\n",
    "qc_report = quick_postcheck(df_v3, target_col=TARGET)\n",
    "\n",
    "print(f\"Dataset v3 shape: {qc_report['shape']}\")\n",
    "print(f\"Total NaNs: {qc_report['nans_total']} ({qc_report['nan_rate_pct']:.2f}%)\")\n",
    "print(f\"Duplicates by id+rn: {qc_report['duplicates']}\")\n",
    "print(\"Target distribution:\", qc_report.get(\"target_dist\", {}))\n",
    "print(\"Top 10 NaN-rate columns (%):\", qc_report[\"top10_null_cols\"])\n",
    "\n",
    "# Save quick summary to report JSON\n",
    "quick_report_path = REPORTS_DIR / \"03_2_final_dataset_v3_quickcheck.json\"\n",
    "with open(quick_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qc_report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved:\", quick_report_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9228cdc8",
   "metadata": {},
   "source": [
    "## 08. Extended checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a37e4",
   "metadata": {},
   "source": [
    "**Purpose**: Run extended validation — correlation snapshot, identical columns scan, and basic schema metadata (safe & sampled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "44f2b0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended checks saved → D:\\final_v2\\credit-risk-management\\reports\\03_2_final_dataset_v3_extcheck.json\n",
      "Top 30 corr pairs (abs): [('paym_ok_share_3', 'paym_late_share_3', 1.0), ('paym_ok_share_24', 'paym_late_share_24', 1.0), ('paym_ok_share_delta_6_12', 'paym_late_momentum_12', 1.0), ('outstanding_to_limit', 'credit_headroom', 1.0), ('enc_paym_0', 'paym_last_status', 1.0)]\n",
      "Identical groups found: 3 (showing up to 20 in report)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "# Config for safe computation on large frames\n",
    "EXT_SAMPLE_ROWS = 200_000   # sample for correlation to avoid OOM\n",
    "TOP_K_CORR_PAIRS = 30       # how many top correlated pairs to keep\n",
    "CORR_THRESHOLD = 0.98       # flag pairs with |corr| >= threshold\n",
    "\n",
    "def sample_df(df: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    if len(df) <= n:\n",
    "        return df\n",
    "    return df.sample(n=n, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "def top_correlated_pairs(df: pd.DataFrame, k: int, thr: float) -> List[Dict[str, Any]]:\n",
    "    num_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if not num_cols:\n",
    "        return []\n",
    "    df_s = sample_df(df[num_cols], EXT_SAMPLE_ROWS)\n",
    "    corr = df_s.corr(numeric_only=True).abs()\n",
    "    # extract upper triangle without diagonal\n",
    "    pairs = []\n",
    "    cols = corr.columns.tolist()\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            c = corr.iat[i, j]\n",
    "            if not np.isnan(c):\n",
    "                pairs.append((cols[i], cols[j], float(c)))\n",
    "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    out = []\n",
    "    for a, b, v in pairs[:k]:\n",
    "        out.append({\"col_a\": a, \"col_b\": b, \"abs_corr\": v, \"flag_high\": v >= thr})\n",
    "    return out\n",
    "\n",
    "def find_identical_columns(df: pd.DataFrame, ignore_cols: List[str]) -> List[List[str]]:\n",
    "    \"\"\"Group columns that are byte-identical (after casting NaN to a sentinel).\"\"\"\n",
    "    cols = [c for c in df.columns if c not in ignore_cols]\n",
    "    sig2cols: Dict[str, List[str]] = {}\n",
    "    for c in cols:\n",
    "        s = df[c]\n",
    "        # normalize to bytes signature\n",
    "        vals = s.fillna(np.nan).to_numpy()\n",
    "        try:\n",
    "            data = vals.tobytes()\n",
    "        except Exception:\n",
    "            # fallback: convert to string (slower but safe)\n",
    "            data = \"|\".join(map(str, s.fillna(\"NaN\").tolist())).encode(\"utf-8\")\n",
    "        key = md5(data).hexdigest()\n",
    "        sig2cols.setdefault(key, []).append(c)\n",
    "    # keep only groups with 2+ members\n",
    "    return [v for v in sig2cols.values() if len(v) >= 2]\n",
    "\n",
    "ext_report: Dict[str, Any] = {}\n",
    "\n",
    "# Basic schema meta\n",
    "ext_report[\"n_rows\"] = int(len(df_v3))\n",
    "ext_report[\"n_cols\"] = int(df_v3.shape[1])\n",
    "ext_report[\"numeric_cols\"] = int(df_v3.select_dtypes(include=[\"number\"]).shape[1])\n",
    "ext_report[\"object_cols\"]  = int(df_v3.select_dtypes(include=[\"object\"]).shape[1])\n",
    "\n",
    "# Correlation snapshot (safe sample)\n",
    "t0 = time.time()\n",
    "ext_report[\"top_corr_pairs\"] = top_correlated_pairs(df_v3.drop(columns=[\"id\",\"rn\",\"target\"], errors=\"ignore\"),\n",
    "                                                    k=TOP_K_CORR_PAIRS, thr=CORR_THRESHOLD)\n",
    "ext_report[\"corr_snapshot_time_sec\"] = round(time.time() - t0, 2)\n",
    "\n",
    "# Identical columns (byte-level)\n",
    "t0 = time.time()\n",
    "ident_groups = find_identical_columns(df_v3.drop(columns=[\"id\",\"rn\",\"target\"], errors=\"ignore\"),\n",
    "                                      ignore_cols=[])\n",
    "ext_report[\"identical_column_groups\"] = ident_groups[:20]  # limit in report\n",
    "ext_report[\"identical_scan_time_sec\"] = round(time.time() - t0, 2)\n",
    "\n",
    "# Save extended report\n",
    "ext_report_path = REPORTS_DIR / \"03_2_final_dataset_v3_extcheck.json\"\n",
    "with open(ext_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ext_report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Extended checks saved → {ext_report_path}\")\n",
    "print(f\"Top {len(ext_report['top_corr_pairs'])} corr pairs (abs):\",\n",
    "      [(p['col_a'], p['col_b'], round(p['abs_corr'],4)) for p in ext_report['top_corr_pairs'][:5]])\n",
    "print(f\"Identical groups found: {len(ident_groups)} (showing up to 20 in report)\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7382d",
   "metadata": {},
   "source": [
    "## 09. Save dataset v3 (+ meta & feature inventory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f27e98",
   "metadata": {},
   "source": [
    "**Purpose**: Apply sentinel imputation, prune redundant features, and persist v3 dataset with metadata & feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bf76e018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned 35 redundant features; 65 remain.\n"
     ]
    }
   ],
   "source": [
    "# --- Sentinel imputation for recency features (v3) ---\n",
    "RECENCY_SENTINEL = 25\n",
    "for col in [\"paym_last_late_recency\", \"paym_last_ok_recency\"]:\n",
    "    if col in df_v3.columns:\n",
    "        df_v3[col] = df_v3[col].fillna(RECENCY_SENTINEL).astype(\"float32\")\n",
    "\n",
    "# --- Feature pruning (v3) ---\n",
    "drop_cols = []\n",
    "\n",
    "# 1) Антикоррелирующие shares: сносим OK-shares полностью\n",
    "drop_cols += [c for c in df_v3.columns if c.startswith(\"paym_ok_share_\")]\n",
    "\n",
    "# 2) Оставляем компактный поднабор поздних share/стрик/трендов/recency (как в 03.1)\n",
    "keep_late_shares = {\"paym_late_share_6\", \"paym_late_share_24\"}\n",
    "keep_streaks     = {\"paym_longest_late_streak_24\"}\n",
    "keep_trend       = {\"paym_ok_trend_6\"}\n",
    "keep_recency     = {\"paym_last_late_recency\"}\n",
    "\n",
    "# 3) Удаляем все paym_* в этих группах, кроме keep_set\n",
    "keep_set = keep_late_shares | keep_streaks | keep_trend | keep_recency\n",
    "drop_cols += [\n",
    "    c for c in df_v3.columns\n",
    "    if (\n",
    "        c.startswith(\"paym_late_share_\")\n",
    "        or c.startswith(\"paym_longest_\")\n",
    "        or c.startswith(\"paym_ok_trend_\")\n",
    "        or c.startswith(\"paym_last_\")\n",
    "    ) and c not in keep_set\n",
    "]\n",
    "\n",
    "# 4) Дубликаты/идентичные колонки по отчёту ext-check → удаляем util_x_overdue\n",
    "if \"util_x_overdue\" in df_v3.columns:\n",
    "    drop_cols.append(\"util_x_overdue\")   # identical to overdue_to_limit (see ext-check)\n",
    "\n",
    "# 5) Сырые enc_paym_*: оставляем первые три\n",
    "paym_raw_cols = sorted([c for c in df_v3.columns if c.startswith(\"enc_paym_\")],\n",
    "                       key=lambda s: int(s.split(\"_\")[-1]))\n",
    "drop_cols += paym_raw_cols[3:]  # keep enc_paym_0..2\n",
    "\n",
    "# 6) Применяем с учётом новых групп (НЕ трогаем transitions/std/any_late/decay/deltas/ratios/logs/util_x_outstanding)\n",
    "before_cols = df_v3.shape[1]\n",
    "df_v3.drop(columns=list(set(drop_cols)), inplace=True, errors=\"ignore\")\n",
    "after_cols = df_v3.shape[1]\n",
    "print(f\"Pruned {before_cols - after_cols} redundant features; {after_cols} remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3864d79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final_dataset_v3.parquet | (3000000, 65) in 4.95s\n",
      "Saved feature list: D:\\final_v2\\credit-risk-management\\reports\\final_v3_feature_list.json\n",
      "Saved meta report: D:\\final_v2\\credit-risk-management\\reports\\final_v3_meta.json\n"
     ]
    }
   ],
   "source": [
    "# --- Save dataset v3 ---\n",
    "t0 = time.time()\n",
    "df_v3.to_parquet(FINAL_DS_V3_PATH, index=False)\n",
    "save_time = round(time.time() - t0, 2)\n",
    "print(f\"Saved {FINAL_DS_V3_PATH.name} | {df_v3.shape} in {save_time}s\")\n",
    "\n",
    "# --- Save feature inventory (v3) ---\n",
    "feature_list = [c for c in df_v3.columns if c not in [\"id\", \"rn\", \"target\"]]\n",
    "with open(FEATURE_LIST_V3, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sorted(feature_list), f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved feature list: {FEATURE_LIST_V3}\")\n",
    "\n",
    "# --- Save metadata report (v3) ---\n",
    "meta = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"seed\": SEED,\n",
    "    \"n_rows\": int(len(df_v3)),\n",
    "    \"n_features\": len(feature_list),\n",
    "    \"save_time_sec\": save_time,\n",
    "    \"removed_features\": sorted(list(set(drop_cols))),\n",
    "    \"recency_sentinel\": RECENCY_SENTINEL,\n",
    "    \"lib_versions\": lib_versions(),\n",
    "}\n",
    "with open(META_V3_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved meta report: {META_V3_JSON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9b433",
   "metadata": {},
   "source": [
    "## 10. Changelog: v1 vs v2 feature diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817bc47",
   "metadata": {},
   "source": [
    "**Purpose**: compare feature inventory between v2 and v3 datasets and generate a lightweight changelog report (counts, lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "afcb5965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved changelog → D:\\final_v2\\credit-risk-management\\reports\\final_v3_changelog.json\n",
      "v2→v3: +34 / -0 | common=28\n"
     ]
    }
   ],
   "source": [
    "def load_v2_columns(path: Path) -> List[str]:\n",
    "    df = pd.read_parquet(path, columns=None)\n",
    "    return df.columns.tolist()\n",
    "\n",
    "# Load v2 columns (light operation)\n",
    "v2_cols = load_v2_columns(FINAL_DS_V2_PATH)\n",
    "v3_cols = df_v3.columns.tolist()\n",
    "\n",
    "# Keep only feature columns (exclude id/rn/target)\n",
    "def feat_only(cols: List[str]) -> List[str]:\n",
    "    return [c for c in cols if c not in (\"id\", \"rn\", \"target\")]\n",
    "\n",
    "v2_feats = set(feat_only(v2_cols))\n",
    "v3_feats = set(feat_only(v3_cols))\n",
    "\n",
    "added    = sorted(v3_feats - v2_feats)\n",
    "removed  = sorted(v2_feats - v3_feats)\n",
    "common   = sorted(v2_feats & v3_feats)\n",
    "\n",
    "changelog_v3 = {\n",
    "    \"v2_feature_count\": len(v2_feats),\n",
    "    \"v3_feature_count\": len(v3_feats),\n",
    "    \"added_count\": len(added),\n",
    "    \"removed_count\": len(removed),\n",
    "    \"common_count\": len(common),\n",
    "    \"added\": added[:200],\n",
    "    \"removed\": removed[:200],\n",
    "}\n",
    "\n",
    "change_path_v3 = REPORTS_DIR / \"final_v3_changelog.json\"\n",
    "with open(change_path_v3, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(changelog_v3, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved changelog → {change_path_v3}\")\n",
    "print(f\"v2→v3: +{len(added)} / -{len(removed)} | common={len(common)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142ecb3",
   "metadata": {},
   "source": [
    "## 11. Sanity importance (light RandomForest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f375bad2",
   "metadata": {},
   "source": [
    "**Purpose**: Train a lightweight RandomForest on a sampled subset to estimate relative feature importances (quick signal check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e9d8986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sanity RF] AUC=0.6449 | Features: 62 | Sample: 200000 | Saved top-20 → final_v3_feature_importance_rf_sanity.json\n",
      "enc_paym_1                          0.0835\n",
      "enc_paym_2                          0.0521\n",
      "enc_paym_0                          0.0501\n",
      "paym_late_time_decay_07             0.0403\n",
      "paym_last_late_recency              0.0383\n",
      "pre_loans_max_overdue_sum           0.0327\n",
      "paym_late_time_decay                0.0327\n",
      "maxover_to_limit                    0.0306\n",
      "paym_transitions_24                 0.0299\n",
      "outstanding_to_limit                0.0275\n",
      "util_log1p                          0.0266\n",
      "util_x_outstanding                  0.0264\n",
      "pre_since_opened                    0.0257\n",
      "age_since_opened_minus_confirmed    0.0256\n",
      "paym_any_late_3                     0.0256\n",
      "credit_headroom                     0.0255\n",
      "paym_late_time_decay_088            0.0253\n",
      "late6_x_util                        0.0253\n",
      "pre_pterm                           0.0250\n",
      "maxover_to_outstanding              0.0250\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "SAMPLE_SIZE = 200_000 if len(df_v3) > 200_000 else len(df_v3)\n",
    "df_sample = df_v3.sample(SAMPLE_SIZE, random_state=SEED)\n",
    "\n",
    "X = df_sample.drop(columns=[\"id\", \"rn\", \"target\"], errors=\"ignore\")\n",
    "y = df_sample[\"target\"].astype(int)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=8,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=SEED,\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict_proba(X_valid)[:, 1]\n",
    "auc = roc_auc_score(y_valid, pred)\n",
    "\n",
    "imp = (\n",
    "    pd.Series(rf.feature_importances_, index=X.columns)\n",
    "      .sort_values(ascending=False)\n",
    "      .head(20)\n",
    "      .round(4)\n",
    ")\n",
    "\n",
    "# save\n",
    "imp_path = REPORTS_DIR / \"final_v3_feature_importance_rf_sanity.json\"\n",
    "imp.to_json(imp_path, indent=2)\n",
    "\n",
    "print(f\"[Sanity RF] AUC={auc:.4f} | Features: {X.shape[1]} | Sample: {SAMPLE_SIZE} | Saved top-20 → {imp_path.name}\")\n",
    "print(imp)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80fe5d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RF AUC: 0.6360  | Features: 62  | Sample: 200000\n",
      "             group  n_cols  dAUC_vs_base\n",
      "7     interactions       1      -0.00017\n",
      "5             logs       3      -0.00016\n",
      "2         any_late       4       0.00123\n",
      "0      transitions       4       0.00129\n",
      "1         late_std       4       0.00151\n",
      "6     ratios_extra       1       0.00197\n",
      "3       time_decay       1       0.00209\n",
      "4  ok_share_deltas       0           NaN\n"
     ]
    }
   ],
   "source": [
    "# Purpose: Run remove-one-group ablation on df_v3 to see which new groups matter for AUC (same sample/split across runs).\n",
    "\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# --- fixed sample/split for comparability ---\n",
    "SAMPLE_SIZE = 200_000 if len(df_v3) > 200_000 else len(df_v3)\n",
    "df_s = df_v3.sample(SAMPLE_SIZE, random_state=SEED)\n",
    "X_full = df_s.drop(columns=[\"id\",\"rn\",\"target\"], errors=\"ignore\")\n",
    "y_full = df_s[\"target\"].astype(int)\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X_full, y_full, test_size=0.25, stratify=y_full, random_state=SEED)\n",
    "\n",
    "def rf_auc(X_train, X_valid, y_train, y_valid):\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, class_weight=\"balanced\", random_state=SEED)\n",
    "    rf.fit(X_train, y_train)\n",
    "    return roc_auc_score(y_valid, rf.predict_proba(X_valid)[:,1])\n",
    "\n",
    "# --- define groups by patterns (v3 add-ons) ---\n",
    "groups = {\n",
    "    \"transitions\":       [c for c in X_full.columns if re.match(r\"^paym_transitions_\\d+$\", c)],\n",
    "    \"late_std\":          [c for c in X_full.columns if re.match(r\"^paym_late_std_\\d+$\", c)],\n",
    "    \"any_late\":          [c for c in X_full.columns if re.match(r\"^paym_any_late_\\d+$\", c)],\n",
    "    \"time_decay\":        [c for c in X_full.columns if c == \"paym_late_time_decay\"],\n",
    "    \"ok_share_deltas\":   [c for c in X_full.columns if re.match(r\"^paym_ok_share_delta_\\d+_\\d+$\", c)],\n",
    "    \"logs\":              [c for c in X_full.columns if c.endswith(\"_log1p\")],\n",
    "    \"ratios_extra\":      [c for c in X_full.columns if c in {\"overdue_ratio\"}],  # keep simple\n",
    "    \"interactions\":      [c for c in X_full.columns if re.match(r\"^util_x_(overdue|outstanding)$\", c)],\n",
    "}\n",
    "\n",
    "# baseline\n",
    "auc_base = rf_auc(X_tr, X_va, y_tr, y_va)\n",
    "print(f\"Baseline RF AUC: {auc_base:.4f}  | Features: {X_full.shape[1]}  | Sample: {SAMPLE_SIZE}\")\n",
    "\n",
    "# remove-one\n",
    "rows = []\n",
    "for name, cols in groups.items():\n",
    "    if not cols: \n",
    "        rows.append((name, 0, None))\n",
    "        continue\n",
    "    keep = [c for c in X_full.columns if c not in cols]\n",
    "    Xtr_k, Xva_k = X_tr[keep], X_va[keep]\n",
    "    auc_k = rf_auc(Xtr_k, Xva_k, y_tr, y_va)\n",
    "    rows.append((name, len(cols), round(auc_k - auc_base, 5)))\n",
    "\n",
    "ablation = pd.DataFrame(rows, columns=[\"group\",\"n_cols\",\"dAUC_vs_base\"]).sort_values(\"dAUC_vs_base\")\n",
    "print(ablation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee68249",
   "metadata": {},
   "source": [
    "## 12. Sanity importance (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "010f440c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LGBM sanity] AUC=0.6452 | Features: 62 | Sample: 200000 | Saved → final_v3_lgbm_sanity_importance_top30.json\n",
      "age_since_opened_minus_confirmed    1837\n",
      "outstanding_to_limit                1318\n",
      "pre_pterm                           1248\n",
      "pre_fterm                           1240\n",
      "pre_since_confirmed                 1191\n",
      "late6_x_util                        1129\n",
      "pre_till_pclose                     1079\n",
      "pre_till_fclose                      938\n",
      "maxover_to_limit                     907\n",
      "pre_since_opened                     853\n",
      "pre_loans_credit_limit               847\n",
      "maxover_to_outstanding               463\n",
      "paym_late_std_24                     429\n",
      "paym_late_momentum_24                365\n",
      "paym_last_late_recency               362\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# Purpose: Quick LightGBM sanity on the same sample/split; report AUC and top importances.\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# reuse the same df_s, X_tr, X_va, y_tr, y_va from ablation cell\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"auc\",\n",
    "    verbose=-1\n",
    ")\n",
    "lgbm.fit(X_tr, y_tr, eval_set=[(X_va, y_va)])\n",
    "auc_lgb = roc_auc_score(y_va, lgbm.predict_proba(X_va)[:,1])\n",
    "\n",
    "imp_lgb = (\n",
    "    pd.Series(lgbm.feature_importances_, index=X_tr.columns)\n",
    "      .sort_values(ascending=False).head(30)\n",
    ")\n",
    "\n",
    "imp_path = REPORTS_DIR / \"final_v3_lgbm_sanity_importance_top30.json\"\n",
    "imp_lgb.to_json(imp_path, indent=2)\n",
    "\n",
    "print(f\"[LGBM sanity] AUC={auc_lgb:.4f} | Features: {X_tr.shape[1]} | Sample: {len(X_tr)+len(X_va)} | Saved → {imp_path.name}\")\n",
    "print(imp_lgb.head(15).round(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
